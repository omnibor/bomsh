#! /bin/env python3
# Copyright (c) 2022 Cisco and/or its affiliates.
#
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at:
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Bomsh script to graft/prune artifact trees for OmniBOR software builds.

Based upon the OmniBOR artifact trees generated by Bomsh or Bomtrace.

As a result, multiple OmniBOR bom-ids can be associated with a single
artifact at the same time for a single software build instance.
For example, internal development engineers can have a bom-id which
have all the subtrees of your company's proprietary software, while
external customers only have a bom-id without the subtrees of your
company's proprietary software.

December 2021, Yongkui Han
"""

import argparse
import sys
import os
import shutil
import subprocess
import json

# for special filename handling with shell
try:
    from shlex import quote as cmd_quote
except ImportError:
    from pipes import quote as cmd_quote

TOOL_VERSION = '0.0.1'
VERSION = '%(prog)s ' + TOOL_VERSION

LEVEL_0 = 0
LEVEL_1 = 1
LEVEL_2 = 2
LEVEL_3 = 3
LEVEL_4 = 4
LEVEL_5 = 5

args = None
g_tmpdir = "/tmp"
# the hash-tree DB from bomsh_omnibor_treedb
# { githash of binary file => list of githashes + metadata }
g_treedb = {}

#
# Helper routines
#########################
def verbose(string, level=1, logfile=None):
    """
    Prints information to stdout depending on the verbose level.
    :param string: String to be printed
    :param level: Unsigned Integer, listing the verbose level
    :param logfile: file to write
    """
    if args.verbose >= level:
        if logfile:
            append_text_file(logfile, string + "\n")
        # also print to stdout
        print(string)


def get_or_create_dir(destdir):
    """
    Create a directory if it does not exist. otherwise, return it directly
    return absolute path of destdir
    """
    if destdir and os.path.exists(destdir):
        return os.path.abspath(destdir)
    os.makedirs(destdir)
    return os.path.abspath(destdir)


def write_text_file(afile, text):
    '''
    Write a string to a text file.

    :param afile: the text file to write
    '''
    with open(afile, 'w') as f:
         return f.write(text)


def append_text_file(afile, text):
    '''
    Append a string to a text file.

    :param afile: the text file to write
    '''
    with open(afile, 'a+') as f:
         return f.write(text)


def get_shell_cmd_output(cmd):
    """
    Returns the output of the shell command "cmd".

    :param cmd: the shell command to execute
    """
    #print (cmd)
    output = subprocess.check_output(cmd, shell=True, universal_newlines=True)
    return output


def get_filetype(afile):
    """
    Returns the output of the shell command "file afile".

    :param afile: the file to check its file type
    """
    cmd = "file " + cmd_quote(afile) + " || true"
    #print (cmd)
    output = subprocess.check_output(cmd, shell=True, universal_newlines=True)
    res = re.split(":\s+", output.strip())
    if len(res) > 1:
        return ": ".join(res[1:])
    return "empty"


def load_json_db(db_file):
    """ Load the the data from a JSON file

    :param db_file: the JSON database file
    :returns a dictionary that contains the data
    """
    db = dict()
    with open(db_file, 'r') as f:
        db = json.load(f)
    return db


def save_json_db(db_file, db, indentation=4):
    """ Save the dictionary data to a JSON file

    :param db_file: the JSON database file
    :param db: the python dict struct
    :returns None
    """
    if not db:
        return
    print ("save_json_db: db file is " + db_file)
    try:
        f = open(db_file, 'w')
    except IOError as e:
        print ("I/O error({0}): {1}".format(e.errno, e.strerror))
        print ("Error in save_json_db, skipping it.")
    else:
        with f:
            json.dump(db, f, indent=indentation, sort_keys=True)


def find_all_regular_files_in_dirs(builddirs):
    """
    Find all regular files in the build dirs, excluding symbolic link files.

    It simply runs the shell's find command and saves the result.

    :param builddir: a list of directories
    :returns a list that contains all the regular file names.
    """
    adirs = ' '.join([cmd_quote(adir) for adir in builddirs])
    findcmd = "find " + adirs + ' -type f -print || true '
    output = subprocess.check_output(findcmd, shell=True, universal_newlines=True)
    files = output.splitlines()
    return files


def get_git_file_hash_sha256(afile):
    '''
    Get the git object hash value of a file for SHA256 hash type.
    :param afile: the file to calculate the git hash or digest.
    '''
    cmd = 'printf "blob $(wc -c < ' + afile + ')\\0" | cat - ' + afile + ' | sha256sum | head --bytes=-4 || true'
    #print(cmd)
    output = get_shell_cmd_output(cmd)
    #verbose("output of git_hash_sha256:\n" + output, LEVEL_3)
    if output:
        return output.strip()
    return ''


def get_git_file_hash(afile):
    '''
    Get the git object hash value of a file.
    :param afile: the file to calculate the git hash or digest.
    '''
    if args.hashtype and args.hashtype.lower() == "sha256":
        return get_git_file_hash_sha256(afile)
    cmd = 'git hash-object ' + cmd_quote(afile) + ' || true'
    #print(cmd)
    output = get_shell_cmd_output(cmd)
    #verbose("output of git_hash:\n" + output, LEVEL_3)
    if output:
        return output.strip()
    return ''

############################################################
#### End of helper routines ####
############################################################

def save_gitbom_doc(gitbom_doc_file, destdir, checksum=''):
    '''
    Save the generated OmniBOR doc file to destdir.
    :param gitbom_doc_file: the generated OmniBOR doc file to save
    :param destdir: destination directory to store the created OmniBOR doc file
    :param checksum: the githash of gitbom_doc_file
    '''
    if checksum:
        ahash = checksum
    else:
        ahash = get_git_file_hash(gitbom_doc_file)
    subdir = os.path.join(destdir, ahash[:2])
    object_file = os.path.join(subdir, ahash[2:])
    if os.path.exists(object_file):
        return
    cmd = 'mkdir -p ' + subdir + ' && /usr/bin/cp ' + gitbom_doc_file + ' ' + object_file + ' || true'
    os.system(cmd)


def get_omnibor_doc_first_line():
    """
    Get the first line of OmniBOR doc.
    """
    if args.hashtype and args.hashtype.lower() == "sha256":
        firstline = "gitoid:blob:sha256\n"
    else:
        firstline = "gitoid:blob:sha1\n"
    return firstline


def save_gitbom_doc_lines(lines, destdir):
    '''
    Save the generated OmniBOR doc file to destdir.
    :param lines: the text lines to save
    :param destdir: destination directory to store the created OmniBOR doc file
    returns the bom-id, which is the gitoid of the OmniBOR doc.
    '''
    lines.sort()
    text_lines = '\n'.join(lines) + '\n'
    output_file = os.path.join(g_tmpdir, "bomsh_temp_gitbom_file")
    firstline = get_omnibor_doc_first_line()
    write_text_file(output_file, firstline + text_lines)
    ahash = get_git_file_hash(output_file)
    save_gitbom_doc(output_file, destdir, ahash)
    return ahash


def get_blob_bom_id_from_checksum_line(checksum_line):
    '''
    Extract blob_id, bom_id from the checksum line.
    The checksum line can be: 40-character SHA1 checksum, "blob SHA1", or "blob SHA1 bom SHA1"
    The checksum line can be: 64-character SHA256 checksum, "blob SHA256", or "blob SHA256 bom SHA256"
    :param checksum_line: the checksum line provided
    '''
    if checksum_line[:5] == "blob ":
        if args.hashtype and args.hashtype.lower() == "sha256":
            return (checksum_line[5:69], checksum_line[74:138])
        else:
            return (checksum_line[5:45], checksum_line[50:90])
    if len(checksum_line) in (40, 64) and is_hex_string(checksum_line):
        return (checksum_line, '')
    return ('', '')


def save_omnibor_docs_for_blob_tree_node(tree, checksum, bom_db, destdir, raw_logfile='', index_db=None):
    """
    Generate and save all OmniBOR docs in subtree rooted at checksum to destdir.
    the checksum => bom-id mapping is saved to bom_db when this function returns
    This function recurses on itself.
    :param tree: the constructed bomsh_omnibor_treedb by bomsh_create_bom.py script
    :param checksum: the blob ID of an artifact
    :param bom_db: the dict of { blob_id => bom_id } mappings to update
    :param destdir: the destination directory to save the generated OmniBOR docs and metadata
    :param raw_logfile: the new generated raw_logfile to save if it is non-empty
    :param index_db: the source index db to update, if it is not None
    """
    value = tree[checksum]
    if index_db is not None and "prov_pkg" in value:
        index_db[checksum] = value["prov_pkg"]
    if "hash_tree" not in value:
        bom_db[checksum] = ''
        return
    blobs = value["hash_tree"]
    verbose("checksum: " + checksum + " #hash_tree: " + str(len(blobs)), LEVEL_3)
    # let's construct the text lines for OmniBOR doc
    lines = []
    for blob in blobs:
        if blob not in bom_db:
            if blob in tree:
                # Recurse on this blob node to find its bom-id first
                save_omnibor_docs_for_blob_tree_node(tree, blob, bom_db, destdir, raw_logfile, index_db)
            else:  # treat it as leaf node
                bom_db[blob] = ''
        # this blob already exists in bom_db, which means we have visited this blob node
        if bom_db[blob] and (blob in g_treedb and "prune" not in g_treedb[blob]):  # blob must not have the prune attribute
            line = "blob " + blob + " bom " + bom_db[blob]
        else:
            if args.verbose > 1 and bom_db[blob] and blob in g_treedb and "prune" in g_treedb[blob]:
                verbose("Info: non-leaf blob " + blob + " has prune attribute", LEVEL_2)
            line = "blob " + blob
        lines.append(line)
    # now ready to save OmniBOR doc file and associate bom-id with checksum.
    if len(lines) == 1 and not args.new_omnibor_doc_for_unary_transform:
        blobid, bomid = get_blob_bom_id_from_checksum_line(lines[0])
        if bomid:  # unary transform, reuse bomid by default
            bom_db[checksum] = bomid
            if raw_logfile:  # usually not needed, since they are concatenated directly
                append_raw_logfile_with_build_frag(raw_logfile, tree, checksum, value, blobs)
            return
    bom_db[checksum] = save_gitbom_doc_lines(lines, destdir)
    if raw_logfile:  # usually not needed, since the reconstructed raw_logfile has out-of-order build fragments
        append_raw_logfile_with_build_frag(raw_logfile, tree, checksum, value, blobs)


def append_raw_logfile_with_build_frag(raw_logfile, tree, checksum, value, blobs):
    """
    Append to raw_logfile the build fragment for an outfile checksum in the treedb.
    :param raw_logfile: the reconstructed raw_logfile to write
    :param tree: the constructed bomsh_omnibor_treedb by bomsh_create_bom.py script
    :param checksum: the blob ID of outfile
    :param value: the tree[checksum] value
    :param blobs: list of blob IDs of infiles
    """
    text = get_raw_logfile_build_frag_text(tree, checksum, value, blobs)
    append_text_file(raw_logfile, text)


def get_raw_logfile_build_frag_text(tree, checksum, value, blobs, add_prov_pkg=False):
    """
    Get the text to append to raw_logfile, for the build fragment of an outfile in treedb.
    :param tree: the constructed bomsh_omnibor_treedb by bomsh_create_bom.py script
    :param checksum: the blob ID of outfile
    :param value: the tree[checksum] value
    :param blobs: list of blob IDs of infiles
    returns the lines of text, generated for the build fragment.
    """
    lines = []
    if 'file_path' in value:
        lines.append("outfile: " + checksum + " path: " + value['file_path'])
    else:
        lines.append("outfile: " + checksum)
    for blob in blobs:
        blob_value = tree[blob]
        if add_prov_pkg and 'prov_pkg' in blob_value:
            lines.append("infile: " + checksum + " path: " + blob_value['file_path'] + " prov_pkg: " + blob_value['prov_pkg'])
        else:
            lines.append("infile: " + blob + " path: " + blob_value['file_path'])
    if 'build_cmd' in value:
        lines.append("build_cmd: " + value['build_cmd'])
    if 'build_tool' in value:
        lines.append("build_tool: " + value['build_tool'])
    if 'dyn_libs' in value:
        for blob, path in value['dyn_libs']:
            lines.append("dynlib: " + blob + " path: " + path)
    lines.append("==== End of raw info for this process")
    text = "\n" + '\n'.join(lines) + "\n\n"
    return text


def save_omnibor_docs_for_blob_treedb(treedb, destdir, raw_logfile=None, index_db=None):
    """
    Generate and save all OmniBOR docs in a blob treedb to destdir.
    Tested, this one works well.
    :param tree: the constructed bomsh_omnibor_treedb by bomsh_create_bom.py script
    :param destdir: the new OmniBOR dir to store the generated new docs and metadata
    :param raw_logfile: the reconstructed raw_logfile to write
    :param index_db: the source index db to update, if it is not None
    returns the dict of { blob_id = > bom_id } mappings
    """
    if not isinstance(treedb, dict):
        return
    bom_db = {}
    for checksum in treedb:
        value = treedb[checksum]
        if "hash_tree" not in value:
            bom_db[checksum] = ''
            verbose("leaf Checksum: " + checksum, LEVEL_5)
            continue
        verbose("Recurse on Checksum: " + checksum, LEVEL_4)
        save_omnibor_docs_for_blob_tree_node(treedb, checksum, bom_db, destdir, raw_logfile, index_db)
    verbose("===Done saving generated new OmniBOR docs to objects dir: " + destdir, LEVEL_2)
    concise_bom_db = {k:v for k,v in bom_db.items() if v}
    return concise_bom_db


def get_bomsh_gitoid_treedb_file(bom_dir):
    '''
    Get the bomsh_omnibor_treedb file of an OmniBOR dir.
    :param bom_dir: OmniBOR dir generated by bomsh_create_bom.py script
    returns the treedb file keyed by gitoid/blob-id of artifacts.
    '''
    return os.path.join(bom_dir, "metadata", "bomsh", "bomsh_omnibor_treedb")


def merge_treedb_of_bom_dirs(bom_dirs):
    '''
    Merge the artifact trees of multiple OmniBOR directories.
    :param bom_dirs: a list of OmniBOR dir generated by bomsh_create_bom.py script
    returns a merged artifact treedb.
    '''
    dbfiles = [get_bomsh_gitoid_treedb_file(bom_dir) for bom_dir in bom_dirs]
    db = {}
    for dbfile in dbfiles:
        adb = load_json_db(dbfile)
        db.update(adb)
    return db


def merge_raw_logfile_of_bom_dirs(bom_dirs, destdir):
    '''
    Merge the Bomsh raw_logfile of bom_dirs, and write new raw_logfile to destdir.
    :param bom_dirs: a list of OmniBOR dir generated by bomsh_create_bom.py script
    :param destdir: the new OmniBOR dir to store the generated new docs and metadata
    '''
    raw_logfiles = []
    for bom_dir in bom_dirs:
        raw_logfile = os.path.join(bom_dir, "metadata", "bomsh", "bomsh_hook_raw_logfile")
        if not os.path.exists(raw_logfile):
            verbose("Warning: " + raw_logfile + " is missing from bom_dir " + bom_dir)
            continue
        raw_logfiles.append(raw_logfile)
    if not raw_logfiles:
        return ''
    if len(raw_logfiles) < len(bom_dirs):
        verbose("Warning: " + str(len(bom_dirs) - len(raw_logfiles)) + " raw_logfiles are missing.")
    new_raw_logfile = os.path.join(destdir, "metadata", "bomsh", "bomsh_hook_raw_logfile")
    cmd = "cat " + " ".join(raw_logfiles) + " > " + new_raw_logfile
    os.system(cmd)
    return new_raw_logfile


def merge_two_pkg_dbs(pkg_db, pkg_db2):
    '''
    Merge the pkg nodes of pkg_db2 into pkg_db.
    '''
    for pkg in pkg_db2:
        if pkg in pkg_db:
            blobs, blobs2 = pkg_db[pkg]["blobs"], pkg_db2[pkg]["blobs"]
            for blob in blobs2:
                if blob not in blobs:
                    blobs.append(blob)
        else:
            pkg_db[pkg] = pkg_db2[pkg]


def merge_index_db_of_bom_dirs(bom_dirs, destdir):
    '''
    Merge the pkg/blob DBs of bom_dirs, and write new index_db files to destdir.
    :param bom_dirs: a list of OmniBOR dir generated by bomsh_create_bom.py script
    :param destdir: the new OmniBOR dir to store the generated new docs and metadata
    returns True if any pkg_db or index_db is found, otherwise, returns False
    '''
    verbose("merge_index_db_of_bom_dirs, bom_dirs: " + str(bom_dirs) + " destdir: " + destdir, LEVEL_2)
    pkg_db = {}
    for bom_dir in bom_dirs:
        pkg_dbfile = os.path.join(bom_dir, "metadata", "bomsh", "bomsh_index_pkg_db")
        if not os.path.exists(pkg_dbfile):
            verbose("Warning: " + pkg_dbfile + " is missing from bom_dir " + bom_dir)
            continue
        # need to merge the blobs of the same package
        merge_two_pkg_dbs(pkg_db, load_json_db(pkg_dbfile))
    new_pkg_dbfile = os.path.join(destdir, "metadata", "bomsh", "bomsh_index_pkg_db")
    save_json_db(new_pkg_dbfile, pkg_db)
    index_db = {}
    for bom_dir in bom_dirs:
        index_dbfile = os.path.join(bom_dir, "metadata", "bomsh", "bomsh_index_blob_pkg_db")
        if not os.path.exists(index_dbfile):
            verbose("Warning: " + index_dbfile + " is missing from bom_dir " + bom_dir)
            continue
        index_db.update(load_json_db(index_dbfile))
    new_index_dbfile = os.path.join(destdir, "metadata", "bomsh", "bomsh_index_blob_pkg_db")
    save_json_db(new_index_dbfile, index_db)
    if pkg_db or index_db:
        return True
    return False


def merge_bom_dirs(bom_dirs, destdir):
    '''
    Merge the artifact trees of multiple OmniBOR directories.
    :param bom_dirs: a list of OmniBOR dir generated by bomsh_create_bom.py script
    :param destdir: the new OmniBOR dir to store the generated new docs and metadata
    writes new OmniBOR docs and metadata to destdir.
    '''
    # merge the treedb in reversed chronological order
    treedb = merge_treedb_of_bom_dirs(bom_dirs[::-1])
    global g_treedb
    g_treedb = treedb
    bomsh_objects_dir = get_or_create_dir(os.path.join(destdir, "objects"))
    verbose("===Started saving generated OmniBOR docs to objects dir: " + bomsh_objects_dir, LEVEL_2)
    bomsh_metadata_dir = get_or_create_dir(os.path.join(destdir, "metadata", "bomsh"))
    new_raw_logfile = merge_raw_logfile_of_bom_dirs(bom_dirs, destdir)
    raw_logfile = ''
    if not new_raw_logfile:
        verbose("==Will need to generate new raw_logfile from treedb.", LEVEL_2)
        raw_logfile = os.path.join(destdir, "metadata", "bomsh", "bomsh_hook_raw_logfile")
    # copy merged bomsh_index_*_db files too.
    index_db_done = merge_index_db_of_bom_dirs(bom_dirs, destdir)
    index_db = None
    if not index_db_done:
        verbose("==Will need to generate new index_db from treedb.", LEVEL_2)
        index_db = {}
    bom_db = save_omnibor_docs_for_blob_treedb(treedb, bomsh_objects_dir, raw_logfile, index_db)
    save_json_db(os.path.join(bomsh_metadata_dir, "bomsh_omnibor_treedb"), treedb)
    save_json_db(os.path.join(bomsh_metadata_dir, "bomsh_omnibor_doc_mapping"), bom_db)
    save_json_db(os.path.join(bomsh_metadata_dir, "bomsh_index_blob_pkg_db"), index_db)
    verbose("===Done saving generated metadata docs to metadata dir: " + bomsh_metadata_dir, LEVEL_2)


def prune_bom_dir(bom_dir, blob_ids, destdir):
    '''
    Prune or unprune the artifact tree of an OmniBOR directory.
    :param bom_dir: OmniBOR dir generated by bomsh_create_bom.py script
    :param blob_ids: list of artifact blob IDs to prune subtree
    :param destdir: the new OmniBOR dir to store the generated new docs and metadata
    writes new OmniBOR docs and metadata to destdir.
    '''
    treedb = load_json_db(get_bomsh_gitoid_treedb_file(bom_dir))
    global g_treedb
    g_treedb = treedb
    for blob in blob_ids:
        if blob in treedb:
            value = treedb[blob]
            if "hash_tree" in value:
                # just need to add or delete the prune attribute of the blob node
                if args.unprune:
                    if "prune" in value:  # unpruning, delete the prune attribute
                        del value["prune"]
                        verbose("unpruning non-leaf blob " + blob, LEVEL_2)
                else:  # pruning, add the prune attribute
                    value["prune"] = "subtree"
                    verbose("pruning non-leaf blob " + blob, LEVEL_2)
    bomsh_objects_dir = get_or_create_dir(os.path.join(destdir, "objects"))
    verbose("===Start saving generated OmniBOR docs to objects dir: " + bomsh_objects_dir, LEVEL_2)
    bom_db = save_omnibor_docs_for_blob_treedb(treedb, bomsh_objects_dir)
    bomsh_metadata_dir = get_or_create_dir(os.path.join(destdir, "metadata", "bomsh"))
    save_json_db(os.path.join(bomsh_metadata_dir, "bomsh_omnibor_treedb"), treedb)
    save_json_db(os.path.join(bomsh_metadata_dir, "bomsh_omnibor_doc_mapping"), bom_db)
    # No need to change index_db, but we can modify raw_logfile with the prune attribute
    pkg_dbfile = os.path.join(bom_dir, "metadata", "bomsh", "bomsh_index_pkg_db")
    if os.path.exists(pkg_dbfile):
        shutil.copy(pkg_dbfile, bomsh_metadata_dir)
    index_dbfile = os.path.join(bom_dir, "metadata", "bomsh", "bomsh_index_blob_pkg_db")
    if os.path.exists(index_dbfile):
        shutil.copy(index_dbfile, bomsh_metadata_dir)
    raw_logfile = os.path.join(bom_dir, "metadata", "bomsh", "bomsh_hook_raw_logfile")
    if not os.path.exists(raw_logfile):
        verbose("===There is no existing bomsh raw logfile in bom_dir: " + bom_dir, LEVEL_2)
        return
    new_raw_logfile = os.path.join(bomsh_metadata_dir, "bomsh_hook_raw_logfile")
    create_pruned_raw_logfile(raw_logfile, new_raw_logfile, blob_ids)
    verbose("===Done saving generated metadata docs to metadata dir: " + bomsh_metadata_dir, LEVEL_2)


def create_pruned_raw_logfile(raw_logfile, new_raw_logfile, blob_ids):
    '''
    Create new raw_logfile with some build fragments adding/removing the prune attribute for some blob IDs.
    :param raw_logfile: the old raw_logfile to read
    :param new_raw_logfile: the new raw_logfile to write
    :param blob_ids: list of artifact blob IDs to prune subtree
    '''
    of = open(new_raw_logfile, 'w')
    with open(raw_logfile, 'r') as f:
        found_record = False
        for line in f:
            if line.startswith("outfile: "):
                tokens = line.split()
                if len(tokens) > 3:
                    checksum = tokens[1]
                else:
                    checksum = ''
                if checksum in blob_ids:  # found outfile build record/fragment to truncate
                    found_record = True
            elif found_record and line.startswith("prune: "):
                continue  # skip writing this line, even when pruning, since the prune line will be added at the end
            elif found_record and line.startswith("==== End of raw info for "):
                 found_record = False
                 if not args.unprune:
                     of.write("prune: subtree\n")
            of.write(line)
    of.close()


def prune_bom_dirs(bom_dirs, blob_ids, destdir):
    """
    Prune/Unprune OmniBOR subtrees for a list of blobs.
    If pruning, these blobs will become leaf nodes for their parents even if they have hash_tree.
    If unpruning, these blobs will become intermediate nodes for their parents if they have hash_tree.
    :param bom_dirs: list of OmniBOR dir generated by bomsh_create_bom.py script
    :param blob_ids: list of blob IDs to prune subtrees, or making them leaf nodes
    :param destdir: the new OmniBOR dir to store the generated new docs and metadata
    writes new OmniBOR docs to new dir in destdir.
    """
    if len(bom_dirs) == 1:
        bom_dir = bom_dirs[0]
        new_destdir = os.path.join(destdir, os.path.basename(bom_dir))
        return prune_bom_dir(bom_dir, blob_ids, new_destdir)
    n = 0
    for bom_dir in bom_dirs:
        n += 1
        # use different destdir for each bom_dir to prune
        new_destdir = os.path.join(destdir, os.path.basename(bom_dir) + "-" + str(n))
        prune_bom_dir(bom_dir, blob_ids, new_destdir)

############################################################
#### End of OmniBOR artifact tree graft/prune routines ####
############################################################

def rtd_parse_options():
    """
    Parse command options.
    """
    parser = argparse.ArgumentParser(
        description = "This tool graft/prune OmniBOR artifact trees")
    parser.add_argument("--version",
                    action = "version",
                    version=VERSION)
    parser.add_argument('-B', '--bom_dirs',
                    help = "comma-separated directories storing Bomsh-generated OmniBOR docs and metadata."
                    "If merging, they should be in chronological build order")
    parser.add_argument('-b', '--bom_dir',
                    help = "the single directory to store the generated gitBOM doc files")
    parser.add_argument('--tmpdir',
                    help = "tmp directory, which is /tmp by default")
    parser.add_argument('-O', '--output_dir',
                    help = "the output directory to store generated artifact tree dir")
    parser.add_argument('--hashtype',
                    help = "the hash type, like sha1/sha256, the default is sha1")
    parser.add_argument('-f', '--files',
                    help = "comma-separated files to change embedded bom-id")
    parser.add_argument('-d', '--dirs',
                    help = "comma-separated directories to search for files that need to change embedded bom-id")
    parser.add_argument('-p', '--prune_gitoids',
                    help = "comma-separated blob IDs to prune subtrees")
    parser.add_argument("--unprune",
                    action = "store_true",
                    help = "unprune subtrees instead of pruning")
    parser.add_argument("-g", "--new_omnibor_doc_for_unary_transform",
                    action = "store_true",
                    help = "generate new OmniBOR doc/identifier for single input/output file transform")
    parser.add_argument("-v", "--verbose",
                    action = "count",
                    default = 0,
                    help = "verbose output, can be supplied multiple times"
                           " to increase verbosity")

    # Parse the command line arguments
    args = parser.parse_args()

    if not args.bom_dirs:
        print ("Please specify the BOMSH OmniBOR directory with -B option!")
        print ("")
        parser.print_help()
        sys.exit()

    global g_tmpdir
    if args.tmpdir:
        g_tmpdir = args.tmpdir

    print ("Your command line is:")
    print (" ".join(sys.argv))
    print ("The current directory is: " + os.getcwd())
    print ("")
    return args


def main():
    global args
    # parse command line options first
    args = rtd_parse_options()

    output_dir = "."
    if args.output_dir:
        output_dir = get_or_create_dir(args.output_dir)
    bom_dirs = args.bom_dirs.split(",")
    if args.prune_gitoids:
        prune_bom_dirs(bom_dirs, args.prune_gitoids.split(","), output_dir)
    else:
        merge_bom_dirs(bom_dirs, output_dir)


if __name__ == '__main__':
    main()
