#! /bin/env python3
# Copyright (c) 2022 Cisco and/or its affiliates.
#
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at:
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Bomsh script to search CVEs for binaries or find affected binaries for CVEs.

Based upon the gitBOM artifact tree or gitBOM docs generated by Bomsh or Bomtrace.

December 2021, Yongkui Han
"""

import argparse
import sys
import os
import shutil
import subprocess
import json
import re
try:
    import yaml
except ImportError:
    pass

# for special filename handling with shell
try:
    from shlex import quote as cmd_quote
except ImportError:
    from pipes import quote as cmd_quote

TOOL_VERSION = '0.0.1'
VERSION = '%(prog)s ' + TOOL_VERSION

LEVEL_0 = 0
LEVEL_1 = 1
LEVEL_2 = 2
LEVEL_3 = 3
LEVEL_4 = 4

args = None
# CVE DB with checksum (blob_id) as key, containing various metadata like file_path/CVElist/FixedCVElist, etc.
g_cvedb = {}

# hash-tree DB with checksum (blob_id) as key, "hash_tree" is also a list of checksums (blob_ids)
g_checksum_db = None

# hash-tree DB with bom_id as key, "hash_tree" is a list of checksum_line "blob XX bom YY"
g_bomid_db = None

# metadata DB with checksum (blob_id) as key, containing various metadata like file_path/build_cmd/CVElist, etc.
g_metadata_db = None

# CVE checking rules DB with src_file as key, with value of {cve: {rule_type: afile_rule_value} }
g_cve_check_rules = None

g_tmpdir = "/tmp"
g_jsonfile = "/tmp/bomsh_search_jsonfile"
# All the CVE lists to store CVEs in the search result tree
g_cvelist_keys = ("NoCVElist", "CVElist", "FixedCVElist", "cvehint_CVElist", "cvehint_FixedCVElist")
# the below are metadata keys that should not be recursed by tree recursion
g_metadata_keys = ["file_path", "file_paths", "build_cmd", "cvehints", "swh_path"] + list(g_cvelist_keys)
# The top level software heritage (SWH) directory
g_swh_dir = ''
# a list of source blob files directory for SWH tree
g_swh_src_blob_dir = []

#
# Helper routines
#########################
def verbose(string, level=1, logfile=None):
    """
    Prints information to stdout depending on the verbose level.
    :param string: String to be printed
    :param level: Unsigned Integer, listing the verbose level
    :param logfile: file to write
    """
    if args.verbose >= level:
        if logfile:
            append_text_file(logfile, string + "\n")
        # also print to stdout
        print(string)


def write_text_file(afile, text):
    '''
    Write a string to a text file.

    :param afile: the text file to write
    '''
    with open(afile, 'w') as f:
         return f.write(text)


def append_text_file(afile, text):
    '''
    Append a string to a text file.

    :param afile: the text file to write
    '''
    with open(afile, 'a+') as f:
         return f.write(text)


def read_text_file(afile):
    '''
    Read a text file as a string.

    :param afile: the text file to read
    '''
    with open(afile, 'r') as f:
         return (f.read())


def get_shell_cmd_output(cmd):
    """
    Returns the output of the shell command "cmd".

    :param cmd: the shell command to execute
    """
    #print (cmd)
    output = subprocess.check_output(cmd, shell=True, universal_newlines=True)
    return output


def get_filetype(afile):
    """
    Returns the output of the shell command "file afile".

    :param afile: the file to check its file type
    """
    cmd = "file " + cmd_quote(afile) + " || true"
    #print (cmd)
    output = subprocess.check_output(cmd, shell=True, universal_newlines=True)
    res = re.split(":\s+", output.strip())
    if len(res) > 1:
        return ": ".join(res[1:])
    return "empty"


def load_json_db(db_file):
    """ Load the the data from a JSON file

    :param db_file: the JSON database file
    :returns a dictionary that contains the data
    """
    db = dict()
    with open(db_file, 'r') as f:
        db = json.load(f)
    return db


def save_json_db(db_file, db, indentation=4):
    """ Save the dictionary data to a JSON file

    :param db_file: the JSON database file
    :param db: the python dict struct
    :returns None
    """
    if not db:
        return
    print ("save_json_db: db file is " + db_file)
    try:
        f = open(db_file, 'w')
    except IOError as e:
        print ("I/O error({0}): {1}".format(e.errno, e.strerror))
        print ("Error in save_json_db, skipping it.")
    else:
        with f:
            json.dump(db, f, indent=indentation, sort_keys=True)


def find_all_regular_files_in_dirs(builddirs):
    """
    Find all regular files in the build dirs, excluding symbolic link files.

    It simply runs the shell's find command and saves the result.

    :param builddir: a list of directories
    :returns a list that contains all the regular file names.
    """
    adirs = ' '.join([cmd_quote(adir) for adir in builddirs])
    findcmd = "find " + adirs + ' -type f -print || true '
    output = subprocess.check_output(findcmd, shell=True, universal_newlines=True)
    files = output.splitlines()
    return files


############################################################
#### End of helper routines ####
############################################################


def get_git_file_hash_sha256(afile):
    '''
    Get the git object hash value of a file for SHA256 hash type.
    :param afile: the file to calculate the git hash or digest.
    '''
    cmd = 'printf "blob $(wc -c < ' + afile + ')\\0" | cat - ' + afile + ' | sha256sum | head --bytes=-4 || true'
    #print(cmd)
    output = get_shell_cmd_output(cmd)
    #verbose("output of git_hash_sha256:\n" + output, LEVEL_3)
    if output:
        return output.strip()
    return ''


def get_git_file_hash(afile):
    '''
    Get the git object hash value of a file.
    :param afile: the file to calculate the git hash or digest.
    '''
    if args.hashtype and args.hashtype.lower() == "sha256":
        return get_git_file_hash_sha256(afile)
    cmd = 'git hash-object ' + cmd_quote(afile) + ' || true'
    #print(cmd)
    output = get_shell_cmd_output(cmd)
    #verbose("output of git_hash:\n" + output, LEVEL_3)
    if output:
        return output.strip()
    return ''


def get_git_files_hash(afiles):
    '''
    Get the git hash of a list of files.
    :param afiles: the files to calculate the git hash or digest.
    '''
    hashes = {}
    for afile in afiles:
        hashes[afile] = get_git_file_hash(afile)
    return hashes


def is_archive_file(afile):
    """
    Check if a file is an archive file.

    :param afile: String, name of file to be checked
    :returns True if the file is archive file. Otherwise, return False.
    """
    return get_filetype(afile) == "current ar archive"


def is_jar_file(afile):
    """
    Check if a file is a Java archive file.

    :param afile: String, name of file to be checked
    :returns True if the file is JAR file. Otherwise, return False.
    """
    return " archive data" in get_filetype(afile)


def get_embedded_bom_id_of_archive(afile):
    '''
    Get the embedded 20bytes githash of the associated gitBOM doc for an archive file.
    :param afile: the file to extract the 20-bytes embedded .bom archive entry.
    '''
    abspath = os.path.abspath(afile)
    cmd = 'cd ' + g_tmpdir + ' ; rm -rf .bom ; ar x ' + cmd_quote(abspath) + ' .bom 2>/dev/null || true'
    #print(cmd)
    output = get_shell_cmd_output(cmd)
    bomfile = os.path.join(g_tmpdir, ".bom")
    if os.path.exists(bomfile):
        return get_shell_cmd_output('xxd -p ' + bomfile + ' || true').strip()
    return ''


def get_embedded_bom_id_of_jar_file(afile):
    '''
    Get the embedded 20bytes githash of the associated gitBOM doc for a .jar file.
    :param afile: the file to extract the 20-bytes embedded .bom archive entry.
    '''
    abspath = os.path.abspath(afile)
    cmd = 'cd ' + g_tmpdir + ' ; rm -rf .bom ; jar xf ' + cmd_quote(abspath) + ' .bom 2>/dev/null || true'
    #print(cmd)
    output = get_shell_cmd_output(cmd)
    bomfile = os.path.join(g_tmpdir, ".bom")
    if os.path.exists(bomfile):
        return get_shell_cmd_output('xxd -p ' + bomfile + ' || true').strip()
    return ''


def get_embedded_bom_id_of_elf_file(afile, hash_alg):
    '''
    Get the embedded 20 or 32 bytes githash of the associated gitBOM doc for an ELF file.
    :param afile: the file to extract the embedded .note.omnibor ELF section.
    :param hash_alg: the hashing algorithm, sha1 or sha256
    '''
    abspath = os.path.abspath(afile)
    cmd = 'readelf -x .note.omnibor ' + cmd_quote(afile) + ' 2>/dev/null || true'
    output = get_shell_cmd_output(cmd)
    if not output:
        return ''
    lines = output.splitlines()
    if len(lines) < 4:
        return ''
    result = []
    for line in lines:
        tokens = line.strip().split()
        len_tokens = len(tokens)
        if len_tokens < 2 or tokens[0][:2] != '0x':
            continue
        if len_tokens > 5:
            result.extend( tokens[1:5] )
        else:
            result.extend( tokens[1: len_tokens - 1] )
    if len(result) < 10:
        return ''
    if result[2] == '01000000' and hash_alg == 'sha1':
        return ''.join(result[5:10])
    if result[2] == '02000000' and hash_alg == 'sha256':
        return ''.join(result[5:13])
    if len(result) >= 23 and result[12] == '02000000' and hash_alg == 'sha256':
        return ''.join(result[15:23])
    return ''


def get_embedded_bom_id(afile):
    '''
    Get the embedded 20 or 32 bytes githash of the associated gitBOM doc for a binary file.
    :param afile: the file to extract the 20-bytes embedded .bom section.
    returns a string of 40 characters
    '''
    if args.hashtype and args.hashtype.lower() == "sha256":
        return get_embedded_bom_id_of_elf_file(afile, "sha256")
    else:
        return get_embedded_bom_id_of_elf_file(afile, "sha1")


def get_embedded_bom_id_of_elf_file2(afile):
    '''
    Get the embedded 20bytes githash of the associated gitBOM doc for an ELF file.
    :param afile: the file to extract the 20-bytes embedded .bom ELF section.
    '''
    abspath = os.path.abspath(afile)
    cmd = 'readelf -x .bom ' + cmd_quote(afile) + ' 2>/dev/null || true'
    output = get_shell_cmd_output(cmd)
    if not output:
        return ''
    lines = output.splitlines()
    if len(lines) < 3:
        return ''
    result = []
    for line in lines:
        tokens = line.strip().split()
        if len(tokens) > 5 and tokens[0][:2] == "0x":
            result.extend( (tokens[1], tokens[2], tokens[3], tokens[4]) )
        elif len(tokens) > 2 and tokens[0][:2] == "0x":
            result.append(tokens[1])
            break
    return ''.join(result)


def get_embedded_bom_id2(afile):
    '''
    Get the embedded 20bytes githash of the associated gitBOM doc for a binary file.
    :param afile: the file to extract the 20-bytes embedded .bom section.
    returns a string of 40 characters
    '''
    if is_archive_file(afile):
        return get_embedded_bom_id_of_archive(afile)
    elif is_jar_file(afile):
        return get_embedded_bom_id_of_jar_file(afile)
    else:
        return get_embedded_bom_id_of_elf_file(afile)


# the file blob checksum (blob_id) => gitBOM doc (bom_id) mapping cache DB.
g_gitbom_doc_db = {}
# the gitBOM doc bom_id => gitBOM doc file path mapping cache DB. Needed when gitBOM docs are stored in multiple .omnibor/objects/ directories.
g_gitbom_docfile_db = {}

def create_gitbom_node_of_blob_id(afile):
    '''
    Create the gitBOM hash-tree node from a gitBOM doc.
    :param afile: a single gitBOM doc storing a list of githashes and bom identifiers.
    returns a list of checksum (blob_id).
    '''
    result = []
    content = read_text_file(afile)
    lines = content.splitlines()
    for line in lines:
        tokens = line.split()
        if len(tokens) >= 2 and tokens[0] == "blob":
            result.append(tokens[1])
        if len(tokens) == 4 and tokens[0] == "blob" and tokens[2] == "bom":
            # Add its corresponding gitBOM doc to the cache DB, which is required to find corresponding bom_id for a blob_id
            g_gitbom_doc_db[tokens[1]] = tokens[3]
    return result


def create_gitbom_node_of_bom_id(afile):
    '''
    Create the gitBOM hash-tree node from a gitBOM doc.
    :param afile: a single gitBOM doc storing a list of githashes and bom identifiers.
    returns a list of gitBOM doc (bom_id).
    '''
    result = []
    content = read_text_file(afile)
    lines = content.splitlines()
    for line in lines:
        tokens = line.split()
        if len(tokens) == 2 and tokens[0] == "blob":
            result.append(tokens[1])
        elif len(tokens) == 4 and tokens[0] == "blob" and tokens[2] == "bom":
            result.append(tokens[3])
    return result


def create_gitbom_node_of_checksum_line(afile):
    '''
    Create the gitBOM hash-tree node from a gitBOM doc.
    :param afile: a single gitBOM doc storing a list of githashes and bom identifiers.
    returns a list of checksum lines inside the gitBOM doc.
    '''
    content = read_text_file(afile)
    lines = content.strip().splitlines()
    if lines and (lines[0].startswith("gitoid:blob:sha") or lines[0].startswith("gitoid blob sha")):
        return lines[1:]
    return lines


def get_blob_id_from_checksum_line(checksum_line):
    '''
    Get the gitBOM blob ID from a line in the gitBOM doc.
    The checksum line can be: 40-character SHA1 checksum, "blob SHA1", or "blob SHA1 bom SHA1", or "SHA1"
    The checksum line can be: 64-character SHA256 checksum, "blob SHA256", or "blob SHA256 bom SHA256", or "SHA256"
    :param checksum_line: the checksum line provided
    returns the blob ID
    '''
    if "blob " == checksum_line[:5]:
        if args.hashtype and args.hashtype.lower() == "sha256":
            return checksum_line[5:69]
        else:
            return checksum_line[5:45]
    return checksum_line.strip()


def get_node_id_from_checksum_line(checksum_line):
    '''
    Get the gitBOM hash-tree node ID from a line in the gitBOM doc.
    The checksum line can be: 40-character SHA1 checksum, "blob SHA1", or "blob SHA1 bom SHA1"
    The checksum line can be: 64-character SHA256 checksum, "blob SHA256", or "blob SHA256 bom SHA256"
    :param checksum_line: the checksum line provided
    returns the node ID, which is bom_id if bom_id exists, otherwise, the blob_id
    '''
    if " bom " in checksum_line:
        if args.hashtype and args.hashtype.lower() == "sha256":
            return checksum_line[74:138]
        else:
            return checksum_line[50:90]
    elif "blob " == checksum_line[:5]:
        if args.hashtype and args.hashtype.lower() == "sha256":
            return checksum_line[5:69]
        else:
            return checksum_line[5:45]
    return checksum_line.strip()


def get_all_gitbom_doc_files_in_dir(topdir, is_topdir=True):
    '''
    Get all the gitBOM doc files stored in a directory, which contains multiple .omnibor/objects directories.
    :param topdir: the top directory to store all gitBOM docs
    :param is_topdir: is it the top directory?
    returns a dict of {bomid => gitBOM doc file}
    '''
    hexchar = "[0-9a-f]"
    hexchar_num = 38
    if args.hashtype and args.hashtype.lower() == "sha256":
        hexchar_num = 62
    topdir_abspath = os.path.abspath(topdir)
    if is_topdir:
        #cmd = 'find ' + topdir_abspath + ' -path "*.omnibor/objects/[0-9a-f][0-9a-f]/*" -type f || true'
        # to interoperate with gitbom-llvm implementation
        cmd = 'find ' + topdir_abspath + ' -name "' + hexchar * hexchar_num + '" -path "*.omnibor/objects/[0-9a-f][0-9a-f]/*" -type f || true'
    else:
        cmd = 'find ' + topdir_abspath + ' -name "' + hexchar * hexchar_num + '" -path "*/objects/[0-9a-f][0-9a-f]/*" -type f || true'
    # filter out .git/ directory which contains files with similar names.
    cmd = 'find ' + topdir_abspath + ' -name "' + hexchar * hexchar_num + '" -path "*/[0-9a-f][0-9a-f]/*" -type f | grep -v "\/\.git\/" || true'
    verbose(cmd, LEVEL_3)
    output = get_shell_cmd_output(cmd)
    ret = {}
    if not output:
        return ret
    lines = output.splitlines()
    for line in lines:
        tokens = line.split("/")
        bomid = tokens[-2] + tokens[-1]
        ret[bomid] = line
    return ret


def get_all_gitbom_doc_files(object_bomdir):
    '''
    Get all the gitBOM doc files stored in object_bomdir
    :param object_bomdir: the gitBOM object directory to store all gitBOM docs
    returns a list of gitBOM doc files and its associated githash
    '''
    entries = os.listdir(object_bomdir)
    for entry in entries:
        if not len(entry) == 2:
            continue
        adir = os.path.join(object_bomdir, entry)
        if not os.path.isdir(adir):
            continue
        for afile in os.listdir(adir):
            ahash = entry + afile
            gitbom_doc_file = os.path.join(adir, afile)
            yield gitbom_doc_file, ahash


def create_gitbom_doc_treedb(object_bomdir, use_checksum_line=True):
    '''
    Create the gitBOM doc hash-tree DB from all the gitBOM docs in the bomdir.
    :param object_bomdir: the gitBOM object directory to store all gitBOM docs
    :param use_checksum_line: a flag to use the full checksum line as node of treedb
    returns a dict with bom_id as key (if no bom_id, then use checksum (blob_id) as key)
    '''
    treedb = {}
    if not os.path.isdir(object_bomdir):
        return treedb
    # g_gitbom_docfile_db should have been created
    for ahash in g_gitbom_docfile_db:
        afile = g_gitbom_docfile_db[ahash]
        if use_checksum_line:
            node = create_gitbom_node_of_checksum_line(afile)
        else:
            node = create_gitbom_node_of_bom_id(afile)
        # can only use bom_id as key, since the associated checksum is unknown
        # CVEs are usually associated with leaf nodes only, so this should still work
        treedb[ahash] = {"hash_tree": node}
    return treedb


def update_gitbom_doc_treedb_for_bomid(object_bomdir, checksum, bom_id, treedb):
    '''
    Update/create the gitBOM doc hash-tree DB for a single bom_id, from the gitBOM docs in the bomdir.
    This function recurses on itself.
    :param object_bomdir: the gitBOM object directory to store all gitBOM docs
    :param checksum: the git checksum of the file that is associated with bom_id
    :param bom_id: a single bom_id that is embedded in binary file
    :param treedb: the dict to update with checksum as key
    returns a dict with checksum (blob_id) as key (even if bom_id exists for blob_id)
    '''
    if checksum in treedb:
        return treedb
    afile = os.path.join(object_bomdir, bom_id[:2], bom_id[2:])
    if not os.path.exists(afile):
        return treedb
    node = create_gitbom_node_of_blob_id(afile)
    if not node:
        return treedb
    # Use checksum (not bom_id) as key of treedb, as cvedb is keyed with checksum
    treedb[checksum] = {"hash_tree": node}
    for entry in node:
        if entry in g_gitbom_doc_db:
            update_gitbom_doc_treedb_for_bomid(object_bomdir, entry, g_gitbom_doc_db[entry], treedb)
    return treedb


def get_blob_bom_id_from_checksum_line(checksum_line):
    '''
    Extract blob_id, bom_id from the checksum line.
    The checksum line can be: 40-character SHA1 checksum, "blob SHA1", or "blob SHA1 bom SHA1"
    The checksum line can be: 64-character SHA256 checksum, "blob SHA256", or "blob SHA256 bom SHA256"
    :param checksum_line: the checksum line provided
    '''
    if checksum_line[:5] == "blob ":
        if args.hashtype and args.hashtype.lower() == "sha256":
            return (checksum_line[5:69], checksum_line[74:138])
        else:
            return (checksum_line[5:45], checksum_line[50:90])
    return (checksum_line.strip(), '')


def update_gitbom_doc_treedb_for_checksum_line(object_bomdir, checksum, bom_id, treedb):
    '''
    Update/create the gitBOM doc hash-tree DB for a single bom_id, from the gitBOM docs in the bomdir.
    This function recurses on itself.
    :param object_bomdir: the gitBOM object directory to store all gitBOM docs
    :param checksum: the git checksum (blob_id) of the file that is associated with bom_id
    :param bom_id: a single bom_id that is embedded in binary file
    :param treedb: the dict to update with bom_id as key
    #returns nothing, the treedb parameter is updated
    '''
    if bom_id in treedb:
        return
    if bom_id not in g_gitbom_docfile_db:
        return
    # use g_gitbom_docfile_db to directly get the gitBOM doc file, without using the object_bomdir parameter.
    afile = g_gitbom_docfile_db[bom_id]
    if not os.path.exists(afile):
        return
    node = create_gitbom_node_of_checksum_line(afile)
    if not node:
        return
    # Use bom_id (not blob_id) as key for treedb.
    # CVEs are usually associated with leaf nodes only, so this should still work
    treedb[bom_id] = {"hash_tree": node}
    for line in node:
        blobid, bomid = get_blob_bom_id_from_checksum_line(line)
        if bomid:
            update_gitbom_doc_treedb_for_checksum_line(object_bomdir, blobid, bomid, treedb)
            # well, the below update to g_gitbom_doc_db is not necessary but useful
            if blobid in g_gitbom_doc_db and g_gitbom_doc_db[blobid] != bomid:
                verbose("Warning: blob " + blobid + " has two bom_ids, old: " + g_gitbom_doc_db[blobid] + " new: " + bomid, LEVEL_1)
            g_gitbom_doc_db[blobid] = bomid


def create_gitbom_doc_treedb_for_checksums(bomdir, checksums, use_checksum_line=True):
    '''
    Create the gitBOM doc hash-tree DB for a list of files, from the gitBOM docs in the bomdir.
    :param bomdir: the gitBOM repo directory to store all gitBOM docs and metadata
    :param checksums: a list of checksums (blob IDs) for files, it can be a dict, then its value is its bom_id
    :param use_checksum_line: a flag to use the full checksum line as node of treedb
    returns a dict with bom_id as key (except for the nodes with is_self_hashtree attribute, which has blob_id as key and is top level node only)
    ##returns a dict with checksum (blob_id) as key (even if bom_id exists for blob_id)
    '''
    bom_db = {}
    jsonfile = os.path.join(bomdir, "metadata", "bomsh", "bomsh_omnibor_doc_mapping")
    if os.path.exists(jsonfile):
        bom_db = load_json_db(jsonfile)
    else:
        jsonfile = os.path.join(bomdir, ".omnibor", "metadata", "bomsh", "bomsh_omnibor_doc_mapping")
        if os.path.exists(jsonfile):
            bom_db = load_json_db(jsonfile)
    object_bomdir = os.path.join(bomdir, "objects")
    is_dict = isinstance(checksums, dict)
    treedb = {}
    for checksum in checksums:
        if is_dict:
            bom_id = checksums[checksum]
        else:
            bom_id = ''
        if not bom_id:
            print("Warning: No embedded .bom section in blob ID: " + checksum)
            if bom_db and checksum in bom_db:
                bom_id = bom_db[checksum]
                print("From recorded gitBOM mappings, found bom_id " + bom_id + " for blob ID: " + checksum)
            if not bom_id:
                print("Warning: No recorded bom_id mapping for blob ID: " + checksum)
                continue
        verbose("blob_id: " + checksum + " bom_id: " + bom_id)
        #verbose("blob_id: " + checksum + " bom_id: " + bom_id + " file: " + afile)
        if use_checksum_line:
            # Add below blob_id to bom_id mapping for convenience, which has the is_self_hashtree attribute to distinguish from regular nodes.
            checksum_line = "blob " + checksum + " bom " + bom_id
            treedb[checksum] = {"hash_tree": [checksum_line,], "is_self_hashtree": True}
            # this treedb will use bom_id as node key, except for the above is_self_hashtree node
            update_gitbom_doc_treedb_for_checksum_line(object_bomdir, checksum, bom_id, treedb)
        else:
            # this treedb will use blob_id as node key
            update_gitbom_doc_treedb_for_bomid(object_bomdir, checksum, bom_id, treedb)
    return treedb


def create_gitbom_doc_treedb_for_files(bomdir, afiles, use_checksum_line=True):
    '''
    Create the gitBOM doc hash-tree DB for a list of files, from the gitBOM docs in the bomdir.
    :param bomdir: the gitBOM repo directory to store all gitBOM docs and metadata
    :param afiles: a list of files which may contain embedded .bom section
    :param use_checksum_line: a flag to use the full checksum line as node of treedb
    returns a dict with bom_id as key (except for the nodes with is_self_hashtree attribute, which has blob_id as key and is top level node only)
    ##returns a dict with checksum (blob_id) as key (even if bom_id exists for blob_id)
    '''
    checksums = {get_git_file_hash(afile) : get_embedded_bom_id(afile) for afile in afiles}
    return create_gitbom_doc_treedb_for_checksums(bomdir, checksums, use_checksum_line)


def create_gitbom_doc_treedb_for_files2(bomdir, afiles, use_checksum_line=True):
    '''
    Create the gitBOM doc hash-tree DB for a list of files, from the gitBOM docs in the bomdir.
    :param bomdir: the gitBOM repo directory to store all gitBOM docs and metadata
    :param afiles: a list of files which contain embedded .bom section
    :param use_checksum_line: a flag to use the full checksum line as node of treedb
    returns a dict with bom_id as key (except for the nodes with is_self_hashtree attribute, which has blob_id as key and is top level node only)
    ##returns a dict with checksum (blob_id) as key (even if bom_id exists for blob_id)
    '''
    bom_db = {}
    jsonfile = os.path.join(bomdir, "metadata", "bomsh", "bomsh_omnibor_doc_mapping")
    if os.path.exists(jsonfile):
        bom_db = load_json_db(jsonfile)
    else:
        jsonfile = os.path.join(bomdir, ".omnibor", "metadata", "bomsh", "bomsh_omnibor_doc_mapping")
        if os.path.exists(jsonfile):
            bom_db = load_json_db(jsonfile)
    object_bomdir = os.path.join(bomdir, "objects")
    treedb = {}
    for afile in afiles:
        checksum = get_git_file_hash(afile)
        bom_id = get_embedded_bom_id(afile)
        if not bom_id:
            print("Warning: No embedded .bom section in file: " + afile)
            if bom_db and checksum in bom_db:
                bom_id = bom_db[checksum]
                print("From recorded gitBOM mappings, found bom_id " + bom_id + " for file: " + afile)
            if not bom_id:
                print("Warning: No recorded bom_id mapping for file: " + afile)
                continue
        verbose("blob_id: " + checksum + " bom_id: " + bom_id + " file: " + afile)
        if use_checksum_line:
            # Add below blob_id to bom_id mapping for convenience, which has the is_self_hashtree attribute to distinguish from regular nodes.
            checksum_line = "blob " + checksum + " bom " + bom_id
            treedb[checksum] = {"hash_tree": [checksum_line,], "is_self_hashtree": True}
            # this treedb will use bom_id as node key, except for the above is_self_hashtree node
            update_gitbom_doc_treedb_for_checksum_line(object_bomdir, checksum, bom_id, treedb)
        else:
            # this treedb will use blob_id as node key
            update_gitbom_doc_treedb_for_bomid(object_bomdir, checksum, bom_id, treedb)
    return treedb


def build_swh_source_blob_db_for_dirs(adirs):
    '''
    build the blob ID to file_path mappings for a list of directories.
    :param adirs: a list of directories
    returns a dict of { blob ID => file_path } mappings
    '''
    blob_db = {}
    afiles = find_all_regular_files_in_dirs(adirs)
    for afile in afiles:
        blob_id = get_git_file_hash(afile)
        if blob_id not in blob_db:
            blob_db[blob_id] = afile
    return blob_db


############################################################
#### End of embedded .bom section handling routines ####
############################################################


def get_metadata_for_checksum_from_db(db, checksum, which_list):
    '''
    Get the metadata for a checksum, from the CVE database.
    :param db: the checksum db with metadata
    :param checksum: the checksum provided
    :param which_list: a string for the field name, like file_path
    '''
    if not db or checksum not in db:
        return ''
    entry = db[checksum]
    if which_list in entry:
        return entry[which_list]
    return ''


def is_any_cvelist_in_entry(entry):
    '''
    Is any CVE list in this dict entry.
    '''
    return "NoCVElist" in entry or "CVElist" in entry or "FixedCVElist" in entry or "cvehint_CVElist" in entry or "cvehint_FixedCVElist" in entry


g_checksum_cache_db = {}
def create_hash_tree_for_checksum(checksum, ancestors, checksum_db, checksum_line):
    '''
    Create the hash tree for a checksum, based on the checksum database.
    This function recurses on itself.
    :param checksum: the checksum provided, which should be node_id of checksum_db
    :param ancestors: the list of checksums that are ancestors of this checksum
    :param checksum_db: the checksum database
    :param checksum_line: the checksum line from gitBOM doc for this checksum (node_id)
    returns a multi-tier dict with checksum nodes associated with metadata like CVElist/FixedCVElist
    '''
    # check the cache_db for performance benefit
    if checksum_line in g_checksum_cache_db:
        return g_checksum_cache_db[checksum_line]
    # check for recursion loop
    if checksum in ancestors:
        print("Error in creating hash tree: loop detected for checksum " + checksum + " ancestors: " + str(ancestors))
        return "RECURSION_LOOP_DETECTED"
    entry = {}
    if checksum in checksum_db:
        # Get a shallow copy which should keep metadata like file_path, etc., if it exists
        entry = checksum_db[checksum].copy()
    if "hash_tree" not in entry:  # leaf node
        for which_list in g_cvelist_keys:
            cvelist = get_metadata_for_checksum_from_db(g_cvedb, checksum, which_list)
            if cvelist:
                entry[which_list] = cvelist
            else:
                cvelist = get_metadata_for_checksum_from_db(g_metadata_db, checksum, which_list)
                if cvelist:
                    entry[which_list] = cvelist
        if is_any_cvelist_in_entry(entry) and "file_path" not in entry:
            file_path = get_metadata_for_checksum_from_db(g_cvedb, checksum, "file_path")
            if file_path:
                entry["file_path"] = file_path
        for which_list in ("file_path", "build_cmd"):
            if which_list in entry:
                continue
            metadata = get_metadata_for_checksum_from_db(g_cvedb, checksum, which_list)
            if metadata:
                entry[which_list] = metadata
                continue
            metadata = get_metadata_for_checksum_from_db(g_metadata_db, checksum, which_list)
            if metadata:
                entry[which_list] = metadata
        '''
        if args.software_heritage_save_dir:  # only add swh_path for leaf nodes
            swh_path = os.path.join(args.software_heritage_save_dir, checksum)
            if os.path.exists(swh_path):
                entry['swh_path'] = swh_path
        '''
        # the shallow copy of the checksum node is used, this should be fine
        g_checksum_cache_db[checksum_line] = entry
        return entry
    # non-leaf node, it has more lower level nodes
    hashes = entry["hash_tree"]
    ret = {}
    ancestors.append(checksum)  # add myself to the list of ancestors
    for ahash in set(hashes):
        # ahash is either checksum only or the checksum_line: blob sha1 bom sha1
        node_id = get_node_id_from_checksum_line(ahash)
        # recursion
        ret[ahash] = create_hash_tree_for_checksum(node_id, ancestors, checksum_db, ahash)
    # update metadata for this non-leaf node, based on blob_id
    blob_id, bom_id = get_blob_bom_id_from_checksum_line(checksum_line)
    for which_list in g_cvelist_keys:
        cvelist = get_metadata_for_checksum_from_db(g_cvedb, blob_id, which_list)
        if cvelist:
            ret[which_list] = cvelist
        else:
            cvelist = get_metadata_for_checksum_from_db(g_metadata_db, blob_id, which_list)
            if cvelist:
                ret[which_list] = cvelist
    for key in ("file_path", "file_paths", "build_cmd"):  # try to save more metadata in the result
        if key in entry:
            ret[key] = entry[key]
    if is_any_cvelist_in_entry(ret) and "file_path" not in ret:
        file_path = get_metadata_for_checksum_from_db(g_cvedb, blob_id, "file_path")
        if file_path:
            ret["file_path"] = file_path
    if g_metadata_db:
        for which_list in ("file_path", "build_cmd"):
            if which_list in ret:
                continue
            metadata = get_metadata_for_checksum_from_db(g_metadata_db, blob_id, which_list)
            if metadata:
                ret[which_list] = metadata
    # checksum_line contains both blob_id and bom_id, more accurate/representative than blob_id/bom_id alone
    g_checksum_cache_db[checksum_line] = ret
    ancestors.pop()  # remove myself from the list of ancestors
    return ret


def create_hash_tree_for_checksums(checksums, checksum_db):
    '''
    Create the hash tree for a list of checksums, based on the checksum database.
    :param checksums: the list of checksums provided
    :param checksum_db: the checksum database
    '''
    ret = {}
    for checksum in checksums:
        if checksum not in checksum_db:
            print("Warning: this checksum is not found in checksum DB: " + checksum)
            continue
        ancestors = []  # ancestors is used to detect/stop possible recursion loop
        tree = create_hash_tree_for_checksum(checksum, ancestors, checksum_db, checksum)
        ret[checksum] = tree
    return ret


def collect_cve_list_from_hash_tree(tree_db, which_list):
    '''
    Collect all the CVEs on a hash tree node.
    This function recurses on itself.
    :param tree_db: a node on the hash tree
    :param which_list: a string for the field name, CVEList or FixedCVElist
    '''
    if type(tree_db) is not dict:  # for "NOT_FOUND" or "RECURSION_LOOP_DETECTED"
        return []
    ret = []
    #non_checksum_keys = ("file_path", "file_paths") + g_cvelist_keys
    for checksum in tree_db:
        #if checksum in non_checksum_keys:
        if checksum in g_metadata_keys:
            if checksum == which_list:
                ret.extend(tree_db[checksum])
            continue
        result = collect_cve_list_from_hash_tree(tree_db[checksum], which_list)
        ret.extend(result)
    return list(set(ret))


def find_cve_lists_for_checksums(checksums, is_bom_id=False):
    '''
    find all the CVEs for a list of checksums.
    :param checksums: the list of checksums to find CVEs. either blob_id or bom_id is fine
    '''
    if g_bomid_db:
        tree = create_hash_tree_for_checksums(checksums, g_bomid_db)
    else:
        tree = create_hash_tree_for_checksums(checksums, g_checksum_db)
    if g_jsonfile and args.verbose > 1:
        save_json_db(g_jsonfile + "-details.json", tree)
    if args.copyout_bomdir:  # Copy out necessary gitBOM docs only and truncated metadata files
        copy_all_bomdoc_in_tree(tree, args.copyout_bomdir)
        # Truncate the .omnibor/metadata/bomsh/* files too
        metadata_dir = ""
        if args.bom_topdir:
            metadata_dir = os.path.join(args.bom_topdir, "metadata", "bomsh")
            if not os.path.exists(metadata_dir):
                metadata_dir = os.path.join(args.bom_topdir, ".omnibor", "metadata", "bomsh")
        if not os.path.exists(metadata_dir) and args.bom_dir:
            metadata_dir = os.path.join(args.bom_dir, "metadata", "bomsh")
        if os.path.exists(metadata_dir):
            new_metadata_dir = os.path.join(args.copyout_bomdir, "metadata", "bomsh")
            copy_truncated_metadata_files(tree, new_metadata_dir, metadata_dir)
    if args.subtree_collapsed_bomdir:  # Copy out necessary gitBOM docs only and with subtree-collapsed
        copy_subtree_collapsed_bomdocs_in_tree(tree, args.subtree_collapsed_bomdir)
    if g_swh_dir and args.download_from_swh:
        download_tree_blobs_from_software_heritage(tree, os.path.join(g_swh_dir, "downloads"))
    # Check if there are any new blob IDs that do not exist in g_cvedb
    blobs = check_nonexistent_cve_blob_ids(tree)
    if blobs:
        print("Warning: the CVE search result may be inaccurate, since " + str(len(blobs)) + " blob IDs are not found in CVE DB")
        # Download blobs from softwareHeritage and check against CVE rules
        if g_swh_dir:
            destdir = os.path.join(g_swh_dir, "downloads")
        else:
            destdir = os.path.join(g_tmpdir, "bomsh_swh_save_dir")
        afiles = download_blobs_from_software_heritage(blobs, destdir)
        verbose("For your convenience, " + str(len(afiles)) + " CVEDB-non-existent blob files are downloaded from SoftwareHeritage to directory: " + destdir)
        if afiles and args.cve_check_dir:
            global g_cve_check_rules
            g_cve_check_rules = convert_to_srcfile_cve_rules_db(read_cve_check_rules(args.cve_check_dir))
            cve_results = cve_check_rule_for_files(blobs, afiles, g_cve_check_rules)
            verbose("Here is CVE check results of downloaded SWH blobs: " + json.dumps(cve_results, indent=4, sort_keys=True), LEVEL_3)
            update_hash_tree_with_cve_results(tree, cve_results)
            if g_jsonfile and args.verbose > 1:
                save_json_db(g_jsonfile + "-details_cve_swh.json", tree)
    if g_swh_dir:
        verbose("Updating swh_path for the search result tree...", LEVEL_1)
        blob_db = {}
        if g_swh_src_blob_dir:
            blob_db = build_swh_source_blob_db_for_dirs(g_swh_src_blob_dir)
            if blob_db and args.verbose > 1:
                save_json_db(g_jsonfile + "-source_blob_db.json", blob_db)
        update_tree_blobs_with_software_heritage_path(tree, os.path.join(g_swh_dir, "downloads"), blob_db)
        if g_jsonfile and args.verbose > 1:
            save_json_db(g_jsonfile + "-details_swh.json", tree)
    if args.create_swh_tree_dir:
        verbose("Creating the SWH tree in directory: " + args.create_swh_tree_dir, LEVEL_1)
        create_swh_tree(tree, os.path.abspath(args.create_swh_tree_dir))
    ret = {}
    for checksum in checksums:
        if checksum in tree:
            checksum_result = {}
            for which_list in g_cvelist_keys:
                checksum_result[which_list] = collect_cve_list_from_hash_tree(tree[checksum], which_list)
            ret[checksum] = checksum_result
        else:
            ret[checksum] = "NOT_FOUND_IN_CHECKSUM_DB"
    return ret


def find_cve_lists_for_files(afiles):
    '''
    find all the CVEs for a list of binary files.
    :param afiles: the list of binary files to find CVEs
    '''
    file_checksums = {}
    checksums = []
    for afile in afiles:
        if os.path.exists(afile):
            checksum = get_git_file_hash(afile)
            checksums.append(checksum)
            file_checksums[afile] = checksum
        else:
            file_checksums[afile] = 'FILE_NOT_EXIST'
    result = find_cve_lists_for_checksums(checksums, g_bomid_db)
    for afile in file_checksums:
        checksum = file_checksums[afile]
        if checksum == 'FILE_NOT_EXIST':
            continue
        file_checksums[afile] = result[checksum]
    return file_checksums

############################################################
#### End of CVE handling routines ####
############################################################

def find_vulnerable_blob_ids_for_cve(cve, cve_db):
    '''
    Find all vulnerable git blob IDs for a CVE.
    :param cve_db: the CVE database from the bomsh_create_cve script.
    returns a list of vulnerable git blob IDs for a CVE.
    '''
    result = []
    for blob_id in cve_db:
        entry = cve_db[blob_id]
        if "CVElist" in entry:
            if cve in entry["CVElist"]:
                result.append(blob_id)
    return result


def find_vulnerable_blob_ids_for_cves(cves, cve_db):
    '''
    Find all vulnerable git blob IDs for a list of CVEs.
    :param cve_db: the CVE database from the bomsh_create_cve script.
    returns a list of vulnerable git blob IDs for each CVE.
    '''
    result = {}
    for cve in cves:
        blob_ids = find_vulnerable_blob_ids_for_cve(cve, cve_db)
        result[cve] = blob_ids
    return result


def get_all_src_files_in_cvedb(cve_db):
    '''
    Get all the source code files in CVE database.
    :param cve_db: the CVE database
    returns a list of src files
    '''
    result = set()
    for blob_id in cve_db:
        entry = cve_db[blob_id]
        if "file_path" in entry:
            result.add(entry["file_path"])
    return result


def get_cve_check_source_file(afile, src_files):
    """
    Get the CVE check src_file for a file.
    :param afile: the file to check
    :param src_files: a dict with src_file as key
    """
    for src_file in src_files:
        if afile.endswith(src_file):
            return src_file
    return ''


def get_all_blob_ids_for_src_files_internal(node_key, tree_db, src_files, result):
    '''
    Get all the blob IDs for a list of src_files in the search result tree.
    This function recurses on itself.
    :param node_key: the key associated with tree_db
    :param tree_db: the tree database
    :param src_files: a list of source files to check against
    :param result: the search result to update
    '''
    if not isinstance(tree_db, dict):  # for "NOT_FOUND" or "RECURSION_LOOP_DETECTED"
        return
    #non_checksum_keys = ("file_path", "file_paths") + g_cvelist_keys
    for checksum in tree_db:
        #if checksum in non_checksum_keys:
        if checksum in g_metadata_keys:
            if checksum == "file_path":
                afile = tree_db[checksum]
                src_file = get_cve_check_source_file(afile, src_files)
                if src_file:
                    blob_id, bom_id = get_blob_bom_id_from_checksum_line(node_key)
                    if blob_id in result:
                        old_src_file = result[blob_id][0]
                        if src_file != old_src_file:
                            verbose("Warning: same blob ID " + blob_id + " for two src_files, old: " + old_src_file + " new: " + src_file)
                    else:
                        result[blob_id] = (src_file, afile)
            continue
        # recurse on all its child nodes
        get_all_blob_ids_for_src_files_internal(checksum, tree_db[checksum], src_files, result)


def get_all_blob_ids_for_src_files(tree_db, src_files):
    '''
    Get all the blob IDs for a list of src_files in the search result tree.
    :param tree_db: the tree database
    :param src_files: a list of source files to check against
    returns a dict of {blob_id => (src_file, afile) }
    '''
    result = {}
    get_all_blob_ids_for_src_files_internal('', tree_db, src_files, result)
    return result


def check_nonexistent_cve_blob_ids(tree):
    '''
    Check if there are any blob IDs in the search tree that do not exist in the g_cvedb.
    :param tree: the CVE search details tree DB
    returns a dict of {blob_id => (src_file, afile) } for not-found new blob IDs
    '''
    src_files = get_all_src_files_in_cvedb(g_cvedb)
    verbose("All " + str(len(src_files)) + " CVE src_files: " + str(src_files), LEVEL_3)
    blobs_result = get_all_blob_ids_for_src_files(tree, src_files)
    verbose("All " + str(len(blobs_result)) + " CVE blobs: " + json.dumps(blobs_result, indent=4, sort_keys=True), LEVEL_3)
    ret = {}
    for blob in blobs_result:
        if blob not in g_cvedb:
            ret[blob] = blobs_result[blob]
    verbose("All not-found new CVE blobs: " + json.dumps(ret, indent=4, sort_keys=True), LEVEL_2)
    return ret

############################################################
#### End of CVE finding routines ####
############################################################

def wget_url(url, destdir, destpath=''):
    """
    run wget to download a file from url
    :param url: the URL to download file
    :param destdir: the destination directory to store the downloaded file
    :param destpath: the destination file name or path
    returns the downloaded file path
    """
    #cmd = "wget " + url + " -P " + destdir + " || true"
    #get_shell_cmd_output(cmd)
    if destpath:
        cmds = ['wget', url, '-O', destpath, '--no-check-certificate']
    else:
        cmds = ['wget', url, '-P', destdir, '--no-check-certificate']
        basename = os.path.basename(url)
        if not basename:
            basename = "index.html"
        destpath = os.path.join(destdir, basename)
    retcode = subprocess.run(cmds, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL).returncode
    verbose("retcode: " + str(retcode) + " when downloading URL " + url, LEVEL_4)
    if retcode != 0:
        return ''
    return destpath


# the SoftwareHeritage URL to download a raw blob file is like below:
# https://archive.softwareheritage.org/browse/content/sha1_git:4c376c5b31f5c4ec4c7afb3bbef06fa5ceba4a92/raw/

def download_blob_from_software_heritage(blob, destdir):
    """
    download a file for a blob ID from SoftwareHeritage archive website.
    :param blob: the blob ID to query and download from SoftwareHeritage
    :param destdir: the destination directory to save the downloaded file
    returns the downloaded file path
    """
    url = "https://archive.softwareheritage.org/browse/content/sha1_git:" + blob + "/raw/"
    return wget_url(url, destdir, os.path.join(destdir, blob))


def download_blobs_from_software_heritage(blobs, destdir):
    """
    download files for a list of blob IDs from SoftwareHeritage archive website.
    :param blobs: the blob ID to query and download from SoftwareHeritage
    :param destdir: the destination directory to save the downloaded files
    returns the number of successfully downloaded files
    """
    ret = {}
    if not blobs:
        return ret
    if not os.path.exists(destdir):
        os.system("mkdir -p " + destdir)
    verbose("Trying to download " + str(len(blobs)) + " blobs from SoftwareHeritage to dir: " + destdir, LEVEL_2)
    for blob in blobs:
        afile = download_blob_from_software_heritage(blob, destdir)
        if afile:
            ret[blob] = afile
    verbose("Downloaded " + str(len(ret)) + " blobs from SoftwareHeritage to dir: " + destdir, LEVEL_2)
    return ret


def download_tree_blobs_from_software_heritage(tree, destdir):
    """
    download files for a gitBOM tree from SoftwareHeritage archive website.
    :param tree: the top level tree node of the gitBOM tree with metadata details
    :param destdir: the destination directory to save the downloaded files
    returns the number of successfully downloaded files
    """
    blobs = get_all_blobs_in_tree(tree)
    return download_blobs_from_software_heritage(blobs, destdir)


def update_tree_blobs_with_software_heritage_path(tree, destdir, blob_db, depth=0):
    """
    Update the gitBOM tree with swh_path of downloaded SoftwareHeritage archive.
    :param tree: the top level tree node of the gitBOM tree with metadata details
    :param destdir: the destination directory of the saved downloaded files
    :param blob_db: a dict of { blob_id => file_path } mappings
    :param depth: tree level, top level is depth 0
    This function recurses on itself.
    """
    if not isinstance(tree, dict):
        return
    verbose("\nupdate_tree_blobs_with_swh_path at depth " + str(depth) + " with " + str(len(tree)) + " keys: " + str(list(tree)[:5]), LEVEL_4)
    for checksum_line in tree:
        blob_id = get_blob_id_from_checksum_line(checksum_line)
        if len(blob_id) not in (40, 64):
            continue
        subtree = tree[checksum_line]
        if blob_id in blob_db:
            swh_path = blob_db[blob_id]
            subtree['swh_path'] = swh_path
        else:
            swh_path = os.path.join(destdir, blob_id)
            if os.path.exists(swh_path) and os.path.getsize(swh_path) > 0:
                subtree['swh_path'] = swh_path
        update_tree_blobs_with_software_heritage_path(subtree, destdir, blob_db, depth + 1)


def update_tree_blobs_with_software_heritage_path2(tree, destdir, depth=0):
    """
    Update the gitBOM tree with swh_path of downloaded SoftwareHeritage archive.
    :param tree: the top level tree node of the gitBOM tree with metadata details
    :param destdir: the destination directory of the saved downloaded files
    :param depth: tree level, top level is depth 0
    This function recurses on itself.
    """
    if not isinstance(tree, dict):
        return
    verbose("\nupdate_tree_blobs_with_swh_path at depth " + str(depth) + " with " + str(len(tree)) + " keys: " + str(list(tree)[:5]), LEVEL_4)
    for checksum_line in tree:
        subtree = tree[checksum_line]
        if checksum_line[:5] == "blob " and " bom " not in checksum_line:  # leaf node
            tokens = checksum_line.split()
            swh_path = os.path.join(destdir, tokens[1])
            if os.path.exists(swh_path):
                subtree['swh_path'] = swh_path
            continue
        if depth == 0 or checksum_line[:5] == "blob " and " bom " in checksum_line:  # bom node or top level
           update_tree_blobs_with_software_heritage_path(subtree, destdir, depth + 1)


def create_swh_tree(tree, destdir, depth=0):
    """
    Create the gitBOM tree in a directory with swh_path of downloaded SoftwareHeritage archive as symlinks.
    :param tree: the top level tree node of the gitBOM tree with metadata details
    :param destdir: the destination directory of the saved downloaded files
    :param depth: tree level, top level is depth 0
    This function recurses on itself.
    """
    if not isinstance(tree, dict):
        return
    verbose("\ncreate_swh_tree at depth " + str(depth) + " with " + str(len(tree)) + " keys: " + str(list(tree)[:5]), LEVEL_4)
    for checksum_line in tree:
        subtree = tree[checksum_line]
        newdir = os.path.join(destdir, checksum_line.replace(' ', '-'))
        blob_id = get_blob_id_from_checksum_line(checksum_line)
        if len(blob_id) not in (40, 64):
            # newdir is actually a file, create this new text file
            if checksum_line == 'swh_path':
                cmd = 'ln -sfr ' + cmd_quote(subtree) + ' ' + cmd_quote(newdir) + ' || true'
                os.system(cmd)
            else:
                write_text_file(newdir, str(subtree))
            continue
        if not os.path.exists(newdir):
            os.makedirs(newdir)
        create_swh_tree(subtree, newdir, depth + 1)


def get_all_blobs_in_tree_internal(tree, blob_set):
    """
    Get all unique blob IDs from a gitBOM tree. The node key is checksum_line.
    :param tree: the top level tree node
    :param blob_set: the set to update with blobs in the tree
    """
    for key, value in tree.items():
        if key in g_metadata_keys:
            continue
        blob_id, bom_id = get_blob_bom_id_from_checksum_line(key)
        blob_set.add(blob_id)
        get_all_blobs_in_tree_internal(value, blob_set)


def get_all_blobs_in_tree(tree):
    """
    Get all unique blob IDs from a gitBOM tree. The node key is checksum_line.
    :param tree: the top level tree node of the gitBOM tree
    returns a list of blob IDs
    """
    blob_set = set()
    get_all_blobs_in_tree_internal(tree, blob_set)
    return blob_set


def cve_check_rule_for_files(blobs, afiles, rules):
    """
    Check CVE rules for a list of files.
    :param blobs: dict of {blob => (git_src_file, build_src_file}
    :param afiles: dict of {blob => downloaded_swh_file}
    :param rules: the CVE check rules
    returns the CVE check results
    """
    ret = {}
    for blob in afiles:
        srcfile = blobs[blob][0]
        afile = afiles[blob]
        cve_result = cve_check_rule_for_file(afile, srcfile)
        has_cve_list, fixed_cve_list = get_cvelists_for_cve_result(cve_result)
        ret[blob] = { "file_path": srcfile,
                      "cvehints": cve_result,
                      "cvehint_CVElist": has_cve_list,
                      "cvehint_FixedCVElist": fixed_cve_list}
    return ret


def update_hash_tree_with_cve_results(tree, cve_results):
    """
    Update hash tree with CVE check results
    :param tree: the top level tree node of the gitBOM tree with metadata details
    :param cve_results: the CVE check results
    update the relevant nodes of the tree with the CVE check results
    """
    for key, value in tree.items():
        if key in g_metadata_keys:
            continue
        update_hash_tree_with_cve_results(value, cve_results)
        blob_id, bom_id = get_blob_bom_id_from_checksum_line(key)
        if blob_id not in cve_results:
            continue
        cve_result = cve_results[blob_id]
        for field in ("cvehints", "cvehint_CVElist", "cvehint_FixedCVElist"):
            if field not in value:
                value[field] = cve_result[field]


############################################################
#### End of SofwareHeritage routines ####
############################################################

def read_cve_check_rules(cve_check_dir):
    """
    Read cveadd/cvefix files for the CVE check rules.
    :param cve_check_dir: the directory to store the cveadd/cvefix files.
    returns a dict with all rules.
    """
    ret = {}
    if not os.path.exists(cve_check_dir):
        return ret
    cveadd_file = os.path.join(cve_check_dir, "cveadd")
    cvefix_file = os.path.join(cve_check_dir, "cvefix")
    if not (os.path.exists(cveadd_file) and os.path.exists(cvefix_file)):
        return ret
    ret["cveadd"] = yaml.safe_load(open(cveadd_file, "r"))
    ret["cvefix"] = yaml.safe_load(open(cvefix_file, "r"))
    #print(json.dumps(ret, indent=4, sort_keys=True))
    return ret


def convert_to_srcfile_cve_rules_db(cve_rules_db):
    """
    Convert to srcfile DB from the original cve_rules DB.
    :param cve_rules_db: the original DB read from cve_check_dir files
    returns a new dict with srcfile as key
    """
    ret = {}
    if not cve_rules_db:
        return ret
    for rule_type in ("cveadd", "cvefix"):
        cve_rules = cve_rules_db[rule_type]
        update_srcfile_cve_rules_db(ret, cve_rules, rule_type)
    #print(json.dumps(ret, indent=4, sort_keys=True))
    return ret


def update_srcfile_cve_rules_db(srcfile_db, cve_rules, rule_type):
    """
    Update the srcfile CVE rules DB, from the cve_rules DB with cve as key.
    :param srcfile_db: cve_rules db to update, with src_file as key
    :param cve_rules: cve_rules db, with cve as key
    :param rule_type: cveadd or cvefix
    """
    for cve in cve_rules:
        cve_file_rules = cve_rules[cve]
        for afile in cve_file_rules:
            afile_rule_value = cve_file_rules[afile]
            if afile in srcfile_db:
                srcfile_rules = srcfile_db[afile]
                if cve in srcfile_rules:
                    srcfile_rules[cve][rule_type] = afile_rule_value
                else:
                    srcfile_rules[cve] = {rule_type: afile_rule_value}
            else:
                srcfile_db[afile] = {cve: {rule_type: afile_rule_value} }


def cve_check_rule(afile, rule, content=''):
    """
    Check if a file satisfies a CVE check rule.
    :param afile: the file to check against the CVE rule
    :param rule: the CVE rule to check
    returns True if the rule is satisfied, otherwise, False
    """
    if not content:
        content = read_text_file(afile)
    includes = []
    if "include" in rule:
        includes = rule["include"]
    for string in includes:
        verbose("CVE checking include string: " + str(string), LEVEL_3)
        if isinstance(string, dict):
            for key in string:
                strings = [key,] + string[key]
                if not any_string_in_content(strings, content):
                    return False
        elif string not in content:
            return False
    excludes = []
    if "exclude" in rule:
        excludes = rule["exclude"]
    for string in excludes:
        verbose("CVE checking exclude string: " + str(string), LEVEL_3)
        if isinstance(string, dict):
            for key in string:
                strings = [key,] + string[key]
                if any_string_in_content(strings, content):
                    return False
        elif string in content:
            return False
    return True


def cve_check_rules(afile, rules, content=''):
    ret = {}
    if not content:
        content = read_text_file(afile)
    for rule_type in ("cveadd", "cvefix"):
        if rule_type not in rules:
            continue
        verbose("Checking " + rule_type + " for source file: " + afile, LEVEL_3)
        rule = rules[rule_type]
        ret[rule_type] = cve_check_rule(afile, rule, content)
    return ret


def get_cve_check_source_file(afile, src_files):
    """
    Get the CVE check src_file for a file.
    :param afile: the file to check
    :param src_files: a dict with src_file as key
    """
    for src_file in src_files:
        if afile.endswith(src_file):
            return src_file
    return ''


def get_cvelists_for_cve_result(cve_result):
    """
    Get (has_cve_list, fixed_cve_list) for CVE check result
    :param cve_result: the CVE check result, a dict
    """
    has_cve_list = []
    fixed_cve_list = []
    for cve in cve_result:
        result = cve_result[cve]
        if "cvefix" in result and result["cvefix"]:
            fixed_cve_list.append(cve)
        elif "cveadd" in result and result["cveadd"]:
            has_cve_list.append(cve)
    return (has_cve_list, fixed_cve_list)


def cve_check_rule_for_file(afile, src_file=''):
    """
    Check the CVE rule for a file and return a string for the CVE result
    :param afile: the file to check against the CVE rules
    :param src_file: the git src file for the git blob
    returns a dict keyed with CVE ID
    """
    ret = {}
    if not src_file:
        src_file = get_cve_check_source_file(afile, g_cve_check_rules)
    if not src_file:
        return ''
    content = read_text_file(afile)
    cve_rules = g_cve_check_rules[src_file]
    for cve in cve_rules:
        cve_rule = cve_rules[cve]
        verbose("Checking " + cve + " for source file: " + afile, LEVEL_3)
        ret[cve] = cve_check_rules(afile, cve_rule, content)
    verbose("cve check result for file: " + afile, LEVEL_3)
    verbose(json.dumps(ret, indent=4, sort_keys=True), LEVEL_3)
    return ret


############################################################
#### End of CVE check rules handling routines ####
############################################################

# The below gitBOM doc truncation routines assume g_bomid_db is used, which has bom_id as node key, "hash_tree" is a list of checksum_line "blob XX bom YY"

def copy_truncated_raw_logfile(raw_logfile, blob_ids, outfile_path):
    """
    Create a truncated copy of the bomsh build raw_logfile, based on a list of blob IDs.
    """
    outfile = open(outfile_path, "w")
    #blobs = set(blob_ids)
    write_flag = True
    write_outfiles = 0
    skip_outfiles = 0
    total_lines = 0
    write_lines = 0
    for line in open(raw_logfile, "r"):
        if line[:9] == "outfile: ":
            tokens = line.split()
            blob_id = tokens[1]
            if blob_id in blob_ids:
                write_flag = True
                write_outfiles += 1
            else:
                write_flag = False
                skip_outfiles += 1
            #verbose("outfile blob_id: " + blob_id + " write_flag: " + str(write_flag))
        if write_flag:
            outfile.write(line)
            write_lines += 1
        total_lines += 1
    outfile.close()
    verbose("Truncate raw_logfile: kept " + str(write_outfiles) + " outfiles and deleted " + str(skip_outfiles) + " outfiles, kept " + str(write_lines) + " out of " + str(total_lines) + " total lines")


def copy_truncated_gitbom_doc_mapping(afile, blob_ids, outfile_path, mapping_db={}):
    """
    Create a truncated copy of the bomsh_omnibor_doc_mapping file, blob_id => bom_id
    """
    if mapping_db:
        save_json_db(outfile_path, mapping_db)
        verbose("Truncate gitbom doc mappings: kept " + str(len(mapping_db)) + " blobs")
        return
    db = load_json_db(afile)
    new_db = {}
    for blob_id in db:
        if blob_id in blob_ids:
            new_db[blob_id] = db[blob_id]
    save_json_db(outfile_path, new_db)
    verbose("Truncate gitbom doc mappings: kept " + str(len(new_db)) + " blobs out of total " + str(len(db)) + " blobs")


def copy_truncated_gitbom_treedb(afile, blob_ids, outfile_path):
    """
    Create a truncated copy of the bomsh_omnibor_treedb file, whose node key is blob_id, and node value is hash_tree of list of blob_ids + metadata
    """
    db = load_json_db(afile)
    new_db = {}
    for blob_id in db:
        if blob_id in blob_ids:
            new_db[blob_id] = db[blob_id]
    save_json_db(outfile_path, new_db)
    verbose("Truncate treedb: kept " + str(len(new_db)) + " blobs out of total " + str(len(db)) + " blobs")


def copy_truncated_metadata_files(tree, destdir, metadata_dir):
    """
    Copy truncated bomsh metadata files, based on the CVE search result tree.
    """
    if not os.path.exists(destdir):
        os.makedirs(destdir)
    blob_ids, new_mapping_db = get_all_extra_blobid_in_tree(tree, metadata_dir, True)
    verbose("There are " + str(len(blob_ids)) + " extra blob IDs in the CVE search result")
    raw_logfile = os.path.join(metadata_dir, "bomsh_hook_raw_logfile")
    if os.path.exists(raw_logfile):
        new_raw_logfile = os.path.join(destdir, "bomsh_hook_raw_logfile")
        copy_truncated_raw_logfile(raw_logfile, blob_ids, new_raw_logfile)
    mapping_file = os.path.join(metadata_dir, "bomsh_omnibor_doc_mapping")
    if os.path.exists(mapping_file):
        new_mapping_file = os.path.join(destdir, "bomsh_omnibor_doc_mapping")
        copy_truncated_gitbom_doc_mapping(mapping_file, blob_ids, new_mapping_file, new_mapping_db)
    treedb_file = os.path.join(metadata_dir, "bomsh_omnibor_treedb")
    if os.path.exists(treedb_file):
        new_treedb_file = os.path.join(destdir, "bomsh_omnibor_treedb")
        copy_truncated_gitbom_treedb(treedb_file, blob_ids, new_treedb_file)


def copy_gitbom_doc(srcdoc, destdir, bom_id):
    """
    Copy a single gitBOM doc from srcdir to destdir, preserving the relative path objects/HH/HH38 or /HH/HH62 pattern
    :param srcdoc: the source gitBOM doc, whose filename must be the last 38 characters of the BOM ID.
    :param destdir: the destination directory to copy
    :param bom_id: the 40-character or 64-character BOM ID of the gitBOM doc
    """
    afile = srcdoc
    if not os.path.exists(afile):
        return
    bdir = os.path.join(destdir, "objects", bom_id[:2])
    if not os.path.exists(bdir):
        os.makedirs(bdir)
    if args.remove_sepstrs_in_doc:
        contents = read_text_file(afile).replace("gitoid:blob:sha256\n", "").replace("gitoid:blob:sha1\n", "").replace("blob ", "").replace("bom ", "")
        write_text_file(os.path.join(bdir, bom_id[2:]), contents)
    elif args.insert_sepstrs_in_doc:
        lines = read_text_file(afile).splitlines()
        if not lines:
            shutil.copy(afile, bdir)
            return
        newlines = []
        for line in lines:
            if "blob" not in line:
                newlines.append("blob " + line.replace(" ", " bom "))
            else:
                newlines.append(line)
        firstline = ''
        if not newlines[0].startswith("gitoid:blob:"):
            if len(lines[0]) in (64, 129):
                firstline = "gitoid:blob:sha256\n"
            else:
                firstline = "gitoid:blob:sha1\n"
        write_text_file(os.path.join(bdir, bom_id[2:]), firstline + '\n'.join(newlines) + '\n')
    else:
        shutil.copy(afile, os.path.join(bdir, bom_id[2:]))


def copy_all_bomdoc(docfile_db, destdir):
    """
    Copy all the gitBOM docs to dest directory.
    """
    for bomid in docfile_db:
        copy_gitbom_doc(docfile_db[bomid], destdir, bomid)
    verbose("Copied " + str(len(docfile_db)) + " gitBOM docs to destdir " + destdir)


def copy_subtree_collapsed_bomdoc_in_tree(tree, destdir):
    """
    Collapse all the gitBOM docs of a CVE search result tree to a single level.
    Create a new subtree-collapsed gitBOM doc and copy it to dest directory.
    """
    if not isinstance(tree, dict):
        return ''
    blobids = sorted(get_all_leaf_blobid_in_tree(tree))
    lines = ["blob " + blobid for blobid in blobids]
    if args.hashtype and args.hashtype.lower() == "sha256":
        firstline = "gitoid:blob:sha256\n"
    else:
        firstline = "gitoid:blob:sha1\n"
    content = firstline + '\n'.join(lines) + '\n'
    output_file = os.path.join(g_tmpdir, "bomsh_search_gitbom_file")
    write_text_file(output_file, content)
    ahash = get_git_file_hash(output_file)
    copy_gitbom_doc(output_file, destdir, ahash)
    return ahash


def copy_subtree_collapsed_bomdocs_in_tree(tree, destdir):
    """
    Create new subtree-collapsed gitBOM docs and copy them to dest directory.
    """
    leaf_tree = {}
    for checksum in tree:
        subtree = tree[checksum]
        leaf_tree[checksum] = copy_subtree_collapsed_bomdoc_in_tree(subtree, destdir)
    print("The new subtree collapsed gitBOM doc mapping:")
    print(json.dumps(leaf_tree, indent=4, sort_keys=True))
    return leaf_tree


def copy_all_bomdoc_in_tree(tree, destdir):
    """
    Copy all the gitBOM docs of a CVE search result tree to dest directory.
    """
    bomids = get_all_bomid_in_tree(tree)
    num_copied = 0
    for bomid in bomids:
        if bomid in g_gitbom_docfile_db:
            copy_gitbom_doc(g_gitbom_docfile_db[bomid], destdir, bomid)
            num_copied += 1
        else:
            verbose("Warning: bomid " + bomid + " does not have a gitBOM doc to copy")
    verbose("Total " + str(len(bomids)) + " BOM IDs in the tree, and copied " + str(num_copied) + " gitBOM docs to destdir " + destdir)


def get_all_blob_bomid_in_tree(tree):
    """
    Get a list of blob and BOM IDs for all the nodes in a CVE search result tree.
    This function recurses on itself.
    """
    if not isinstance(tree, dict):
        return []
    result = []
    for node in tree:
        subtree = tree[node]
        blobid, bomid = get_blob_bom_id_from_checksum_line(node)
        if blobid:
            result.append((blobid, bomid))
        result.extend(get_all_blob_bomid_in_tree(subtree))
    return list(set(result))


def get_all_leaf_blobid_in_tree(tree):
    """
    Get a list of leaf blob IDs for all the nodes in a CVE search result tree.
    This function recurses on itself.
    """
    if not isinstance(tree, dict):
        return []
    result = []
    for node in tree:
        subtree = tree[node]
        blobid, bomid = get_blob_bom_id_from_checksum_line(node)
        if blobid and not bomid:
            result.append(blobid)
        result.extend(get_all_blobid_in_tree(subtree))
    return list(set(result))


def get_all_blobid_in_tree(tree):
    """
    Get a list of blob IDs for all the nodes in a CVE search result tree.
    This function recurses on itself.
    """
    if not isinstance(tree, dict):
        return []
    result = []
    for node in tree:
        subtree = tree[node]
        blobid, bomid = get_blob_bom_id_from_checksum_line(node)
        if blobid:
            result.append(blobid)
        result.extend(get_all_blobid_in_tree(subtree))
    return list(set(result))


def get_all_extra_blobid_in_tree(tree, metadata_dir, get_new_mapping=False):
    """
    Get all blob IDs in the tree, plus some extra blob IDs that share the same set of bom IDs in the tree.
    :param tree: the constructed tree for CVE search, with bom_id as node key
    :param metadata_dir: the directory to store bomsh metadata files
    :param get_new_mapping: a flag to get a new mapping db or not
    returns a set of blob IDs, and a new mapping DB dict
    """
    tree_blob_ids = set(get_all_blobid_in_tree(tree))
    # Also get those blob IDs that share the same bom_id in the tree
    bomids = set(get_all_bomid_in_tree(tree))
    mapping_file = os.path.join(metadata_dir, "bomsh_omnibor_doc_mapping")
    new_mapping_db = {}
    if not os.path.exists(mapping_file):
        return (tree_blob_ids, new_mapping_db)
    mapping_db = load_json_db(mapping_file)
    verbose("there are " + str(len(mapping_db)) + " blob_id => bom_id mappings in the original mapping file " + mapping_file)
    blob_ids = set()
    for blob_id in mapping_db:
        bom_id = mapping_db[blob_id]
        if bom_id in bomids:
            blob_ids.add(blob_id)
            if get_new_mapping:
                new_mapping_db[blob_id] = bom_id
    return (blob_ids | tree_blob_ids, new_mapping_db)


def get_all_bomid_in_tree(tree):
    """
    Get a list of BOM IDs for all the nodes in a CVE search result tree.
    This function recurses on itself.
    """
    if not isinstance(tree, dict):
        return []
    result = []
    for node in tree:
        subtree = tree[node]
        blobid, bomid = get_blob_bom_id_from_checksum_line(node)
        if bomid:
            result.append(bomid)
        result.extend(get_all_bomid_in_tree(subtree))
    return list(set(result))


def get_all_bomdoc_in_tree(tree):
    """
    Get a list of gitBOM docs for all the nodes in a CVE search result tree.
    """
    bomids = get_all_bomid_in_tree(tree)
    result = []
    for bomid in bomids:
        if bomid in g_gitbom_docfile_db:
            result.append(g_gitbom_docfile_db[bomid])
    return result

############################################################
#### End of gitBOM doc copy routines ####
############################################################

def rtd_parse_options():
    """
    Parse command options.
    """
    parser = argparse.ArgumentParser(
        description = "This tool searches CVE database and gitBOM database for binary files or CVEs")
    parser.add_argument("--version",
                    action = "version",
                    version=VERSION)
    parser.add_argument('-d', '--cve_db_file',
                    help = "the CVE database file, with git blob ID to CVE mappings")
    parser.add_argument('-r', '--raw_checksums_file',
                    help = "the raw checksum database file generated by bomsh_hook or bomsh_create_bom script")
    parser.add_argument('--bom_topdir',
                    help = "the top directory which usually contains multiple subdirs to store the generated gitBOM doc files")
    parser.add_argument('-b', '--bom_dir',
                    help = "the single directory to store the generated gitBOM doc files")
    parser.add_argument('-e', '--cve_list_to_search',
                    help = "the comma-separated CVE list to search vulnerable git blob IDs")
    parser.add_argument('-c', '--checksums_to_search_cve',
                    help = "the comma-separated git blob ID or checksum list to search CVEs")
    parser.add_argument('-f', '--files_to_search_cve',
                    help = "the comma-separated files to search CVEs")
    parser.add_argument('-g', '--gitbom_ids_to_search_cve',
                    help = "the comma-separated gitBOM ID list to search CVEs")
    parser.add_argument('-m', '--metadata_db_file',
                    help = "the JSON database file containing metadata for file checksums")
    parser.add_argument('--tmpdir',
                    help = "tmp directory, which is /tmp by default")
    parser.add_argument('-j', '--jsonfile',
                    help = "the output JSON file for the search result")
    parser.add_argument('--cve_check_dir',
                    help = "the directory to store all CVE checking rules files")
    parser.add_argument("--software_source_blob_dir",
                    help = "a comma-separated directories with source blob files")
    parser.add_argument("--software_heritage_save_dir",
                    help = "the directory to save the source files if they exist in SoftwareHeritage")
    parser.add_argument("--download_from_swh",
                    action = "store_true",
                    help = "download source blob files from SoftwareHeritage archive website")
    parser.add_argument("--create_swh_tree_dir",
                    help = "create a directory of OmniBOR tree with swh_path symlinks")
    parser.add_argument('--hashtype',
                    help = "the hash type, like sha1/sha256, the default is sha1")
    parser.add_argument('--copyout_bomdir',
                    help = "the new directory to copy out or store the relevant gitBOM doc and metadata files for the search result")
    parser.add_argument('--subtree_collapsed_bomdir',
                    help = "the new directory to copy out or store the relevant subtree-collapsed gitBOM doc and metadata files for the search result")
    parser.add_argument("--remove_sepstrs_in_doc",
                    action = "store_true",
                    help = "remove redundant blob/bom separator strings in gitBOM docs")
    parser.add_argument("--insert_sepstrs_in_doc",
                    action = "store_true",
                    help = "insert redundant blob/bom separator strings in gitBOM docs")
    parser.add_argument("-v", "--verbose",
                    action = "count",
                    default = 0,
                    help = "verbose output, can be supplied multiple times"
                           " to increase verbosity")

    # Parse the command line arguments
    args = parser.parse_args()

    if not (args.raw_checksums_file or args.bom_dir or args.bom_topdir):
        print ("Please specify the BOMSH raw checksum database file with -r option OR the gitBOM directory with -b or --bom_topdir or --bom_dir option!")
        print ("")
        parser.print_help()
        sys.exit()

    global g_jsonfile
    global g_tmpdir
    if args.tmpdir:
        g_tmpdir = args.tmpdir
        g_jsonfile = os.path.join(g_tmpdir, "bomsh_search_jsonfile")
    if args.jsonfile:
        g_jsonfile = args.jsonfile
    global g_swh_dir
    if args.software_heritage_save_dir:
        g_swh_dir = os.path.abspath(args.software_heritage_save_dir)
        if not os.path.exists(g_swh_dir):
            os.makedirs(g_swh_dir)
    if args.software_source_blob_dir:
        g_swh_src_blob_dir.extend([os.path.abspath(adir) for adir in args.software_source_blob_dir.split(",")])

    print ("Your command line is:")
    print (" ".join(sys.argv))
    print ("The current directory is: " + os.getcwd())
    print ("")
    return args


def main():
    global args
    # parse command line options first
    args = rtd_parse_options()

    global g_cvedb
    if args.cve_db_file:
        g_cvedb = load_json_db(args.cve_db_file)
    global g_metadata_db
    if args.metadata_db_file:
        g_metadata_db = load_json_db(args.metadata_db_file)
    elif args.bom_dir:
        bomsh_bomdir = os.path.join(args.bom_dir, "metadata", "bomsh")
        jsonfile = os.path.join(bomsh_bomdir, "bomsh_omnibor_treedb")
        if os.path.exists(jsonfile):
            g_metadata_db = load_json_db(jsonfile)
    global g_checksum_db
    if args.raw_checksums_file:
        g_checksum_db = load_json_db(args.raw_checksums_file)
    elif args.bom_dir or args.bom_topdir:
        global g_bomid_db
        global g_gitbom_docfile_db
        if args.bom_topdir:
            bom_dir = args.bom_topdir
            g_gitbom_docfile_db = get_all_gitbom_doc_files_in_dir(bom_dir)
        else:
            bom_dir = args.bom_dir
            g_gitbom_docfile_db = get_all_gitbom_doc_files_in_dir(bom_dir, False)
        verbose("There are " + str(len(g_gitbom_docfile_db)) + " total gitBOM docs in directory " + bom_dir)
        if args.files_to_search_cve:
            g_bomid_db = create_gitbom_doc_treedb_for_files(bom_dir, args.files_to_search_cve.split(","))
        elif args.checksums_to_search_cve:
            g_bomid_db = create_gitbom_doc_treedb_for_checksums(bom_dir, args.checksums_to_search_cve.split(","))
        else:
            g_bomid_db = create_gitbom_doc_treedb(bom_dir)
    if args.verbose > 2:
        if g_bomid_db:
            save_json_db(g_jsonfile + "-treedb.json", g_bomid_db)
        else:
            save_json_db(g_jsonfile + "-treedb.json", g_checksum_db)
        save_json_db(g_jsonfile + "-bomdocdb.json", g_gitbom_docfile_db)
        save_json_db(g_jsonfile + "-bom-mappings.json", g_gitbom_doc_db)

    cve_result = {}
    if args.files_to_search_cve:
        afiles = args.files_to_search_cve.split(",")
        cve_result = find_cve_lists_for_files(afiles)
    elif args.checksums_to_search_cve:
        checksums = args.checksums_to_search_cve.split(",")
        cve_result = find_cve_lists_for_checksums(checksums, g_bomid_db)
    elif args.cve_list_to_search:
        cves = args.cve_list_to_search.split(",")
        cve_result = find_vulnerable_blob_ids_for_cves(cves, g_cvedb)
    elif args.gitbom_ids_to_search_cve:
        bom_ids = args.gitbom_ids_to_search_cve.split(",")
        cve_result = find_cve_lists_for_checksums(bom_ids, g_bomid_db)
    elif args.copyout_bomdir:  # Copy out all gitBOM docs with remove_sepstrs or insert_sepstrs
        copy_all_bomdoc(g_gitbom_docfile_db, args.copyout_bomdir)
        return
    else:
        print("Did you forget providing files to search?")
        print("Try -c/-f/-g option.")
    if g_jsonfile:
        save_json_db(g_jsonfile, cve_result)
        if args.verbose > 2:
            save_json_db(g_jsonfile + "-cache.json", g_checksum_cache_db)
    print("\nHere is the CVE search results:")
    print(json.dumps(cve_result, indent=4, sort_keys=True))
    return


if __name__ == '__main__':
    main()
